<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-corner-indicator.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Python爬虫一、第一个Python爬虫程序-获取网页html信息1) 获取响应对象向百度（http:&#x2F;&#x2F;www.baidu.com&#x2F;）发起请求，获取百度首页的 HTML 信息，代码如下： 1234#导包,发起请求使用urllib库的request请求模块import urllib.request# urlopen()向URL发请求,返回响应对象,注意url必须完整response&#x3D;urllib">
<meta property="og:type" content="article">
<meta property="og:title" content="Python爬虫教程">
<meta property="og:url" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="破风的博客">
<meta property="og:description" content="Python爬虫一、第一个Python爬虫程序-获取网页html信息1) 获取响应对象向百度（http:&#x2F;&#x2F;www.baidu.com&#x2F;）发起请求，获取百度首页的 HTML 信息，代码如下： 1234#导包,发起请求使用urllib库的request请求模块import urllib.request# urlopen()向URL发请求,返回响应对象,注意url必须完整response&#x3D;urllib">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/image-20221006220332348.png">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/image-20221006220548511.png">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/9-210Q9112J2329.png">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/9-210R0113914536.gif">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/9-210Q9131356309.gif">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/9-210Q91336203T.png">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/9-210Q91336322E.png">
<meta property="og:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/9-210Q9133642311.png">
<meta property="article:published_time" content="2022-10-07T14:53:42.000Z">
<meta property="article:modified_time" content="2022-10-14T14:30:20.764Z">
<meta property="article:author" content="Jiaqi Song">
<meta property="article:tag" content="Python,爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/image-20221006220332348.png">

<link rel="canonical" href="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Python爬虫教程 | 破风的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/PgmTop" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">破风的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-website">

    <a href="/website/login.html" rel="section"><i class="fa fa-website fa-fw"></i>个人网页</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/07/Python%E7%88%AC%E8%99%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/wallhaven-o38w67.jpg">
      <meta itemprop="name" content="Jiaqi Song">
      <meta itemprop="description" content="人生，总会有不期而遇的温暖，和生生不息的希望。不管前方的路有多苦，只要走的方向正确，不管多么崎岖不平，都比站在原地更接近幸福。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="破风的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Python爬虫教程
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-10-07 22:53:42" itemprop="dateCreated datePublished" datetime="2022-10-07T22:53:42+08:00">2022-10-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-10-14 22:30:20" itemprop="dateModified" datetime="2022-10-14T22:30:20+08:00">2022-10-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%88%AC%E8%99%AB/" itemprop="url" rel="index"><span itemprop="name">爬虫</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2022/10/07/Python%E7%88%AC%E8%99%AB/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2022/10/07/Python%E7%88%AC%E8%99%AB/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Python爬虫"><a href="#Python爬虫" class="headerlink" title="Python爬虫"></a>Python爬虫</h1><h1 id="一、第一个Python爬虫程序-获取网页html信息"><a href="#一、第一个Python爬虫程序-获取网页html信息" class="headerlink" title="一、第一个Python爬虫程序-获取网页html信息"></a>一、第一个Python爬虫程序-获取网页html信息</h1><h2 id="1-获取响应对象"><a href="#1-获取响应对象" class="headerlink" title="1) 获取响应对象"></a>1) 获取响应对象</h2><p>向百度（<a target="_blank" rel="noopener" href="http://www.baidu.com/%EF%BC%89%E5%8F%91%E8%B5%B7%E8%AF%B7%E6%B1%82%EF%BC%8C%E8%8E%B7%E5%8F%96%E7%99%BE%E5%BA%A6%E9%A6%96%E9%A1%B5%E7%9A%84">http://www.baidu.com/）发起请求，获取百度首页的</a> HTML 信息，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导包,发起请求使用urllib库的request请求模块</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="comment"># urlopen()向URL发请求,返回响应对象,注意url必须完整response=urllib.request.urlopen(&#x27;http://www.baidu.com/&#x27;)</span></span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<p>上述代码会返回百度首页的响应对象， 其中 urlopen() 表示打开一个网页地址。注意：请求的 url 必须带有 http 或者 https 传输协议。</p>
<p>输出结果，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;http.client.HTTPResponse <span class="built_in">object</span> at <span class="number">0x032F0F90</span>&gt;</span><br></pre></td></tr></table></figure>

<p>上述代码也有另外一种导包方式，也就是使用 from，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#发起请求使用urllib库的request请求模块</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> requestresponse=request.urlopen(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<h2 id="2-输出HTML信息"><a href="#2-输出HTML信息" class="headerlink" title="2) 输出HTML信息"></a>2) 输出HTML信息</h2><p>在上述代码的基础上继续编写如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提取响应内容</span></span><br><span class="line">html = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment">#打印响应内容</span></span><br><span class="line"><span class="built_in">print</span>(html)</span><br></pre></td></tr></table></figure>

<p>输出结果如下，由于篇幅过长，此处只做了简单显示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt; &lt;html&gt;&lt;head&gt;&lt;meta http-equiv=<span class="string">&quot;Content-Type&quot;</span> content=<span class="string">&quot;text/html;charset=utf-8&quot;</span>&gt;&lt;meta http-equiv=<span class="string">&quot;X-UA-Compatible&quot;</span> content=<span class="string">&quot;IE=edge,chrome=1&quot;</span>&gt;&lt;meta content=<span class="string">&quot;always&quot;</span> name=<span class="string">&quot;referrer&quot;</span>&gt;&lt;meta name=<span class="string">&quot;theme-color&quot;</span> content=<span class="string">&quot;#2932e1&quot;</span>&gt;&lt;meta name=<span class="string">&quot;description&quot;</span> content=<span class="string">&quot;全球最大的中文搜索引擎、致力于让网民更便捷地获取信息，找到...&quot;</span>&gt;...&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<p>通过调用 response 响应对象的 read() 方法提取 HTML 信息，该方法返回的结果是字节串类型(bytes)，因此需要使用 decode() 转换为字符串。程序完整的代码程序如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="comment"># urlopen()向URL发请求,返回响应对象</span></span><br><span class="line">response=urllib.request.urlopen(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>)</span><br><span class="line"><span class="comment"># 提取响应内容</span></span><br><span class="line">html = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 打印响应内容</span></span><br><span class="line"><span class="built_in">print</span>(html)</span><br></pre></td></tr></table></figure>

<p>通过上述代码获取了百度首页的 html 信息，这是最简单、最初级的爬虫程序。后续我们还学习如何分析网页结构、解析网页数据，以及存储数据等。</p>
<h2 id="3-常用方法"><a href="#3-常用方法" class="headerlink" title="3) 常用方法"></a>3) 常用方法</h2><p>在本节您认识了第一个爬虫库 urllib，下面关于 urllib 做简单总结。</p>
<h4 id="1-urlopen"><a href="#1-urlopen" class="headerlink" title="1) urlopen()"></a>1) urlopen()</h4><p>表示向网站发起请求并获取响应对象，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlopen(url,timeout)</span><br></pre></td></tr></table></figure>

<p>urlopen() 有两个参数，说明如下：</p>
<ul>
<li>url：表示要爬取数据的 url 地址。</li>
<li>timeout：设置等待超时时间，指定时间内未得到响应则抛出超时异常。</li>
</ul>
<h4 id="2-Request"><a href="#2-Request" class="headerlink" title="2) Request()"></a>2) Request()</h4><p>该方法用于创建请求对象、包装请求头，比如重构 User-Agent（即用户代理，指用户使用的浏览器）使程序更像人类的请求，而非机器。重构 User-Agent 是爬虫和反爬虫斗争的第一步。在下一节会做详细介绍。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.Request(url,headers)</span><br></pre></td></tr></table></figure>

<p>参数说明如下：</p>
<ul>
<li>url：请求的URL地址。</li>
<li>headers：重构请求头。</li>
</ul>
<h4 id="3-html响应对象方法"><a href="#3-html响应对象方法" class="headerlink" title="3) html响应对象方法"></a>3) html响应对象方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">bytes</span> = response.read() <span class="comment"># read()返回结果为 bytes 数据类型</span></span><br><span class="line">string = response.read().decode() <span class="comment"># decode()将字节串转换为 string 类型</span></span><br><span class="line">url = response.geturl() <span class="comment"># 返回响应对象的URL地址</span></span><br><span class="line">code = response.getcode() <span class="comment"># 返回请求时的HTTP响应码</span></span><br></pre></td></tr></table></figure>

<h4 id="4-编码解码操作"><a href="#4-编码解码操作" class="headerlink" title="4) 编码解码操作"></a>4) 编码解码操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#字符串转换为字节码	encode---编码</span></span><br><span class="line">string.encode(<span class="string">&quot;utf-8&quot;</span>) </span><br><span class="line"><span class="comment">#字节码转换为字符串	decode---解码</span></span><br><span class="line"><span class="built_in">bytes</span>.decode(<span class="string">&quot;utf-8&quot;</span>) </span><br></pre></td></tr></table></figure>

<h1 id="二、User-Agent（用户代理）"><a href="#二、User-Agent（用户代理）" class="headerlink" title="二、User-Agent（用户代理）"></a>二、User-Agent（用户代理）</h1><h2 id="1-爬虫程序UA信息"><a href="#1-爬虫程序UA信息" class="headerlink" title="1)爬虫程序UA信息"></a>1)爬虫程序UA信息</h2><p>下面，通过向 HTTP 测试网站（<a target="_blank" rel="noopener" href="http://httpbin.org/%EF%BC%89%E5%8F%91%E9%80%81">http://httpbin.org/）发送</a> GET 请求来查看请求头信息，从而获取爬虫程序的 UA。代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/get&#x27;</span></span><br><span class="line"><span class="comment"># 1、创建请求对象，包装ua信息</span></span><br><span class="line">req = urllib.request.Request(url=url)</span><br><span class="line"><span class="comment"># 向网站发送get请求</span></span><br><span class="line">response = urllib.request.urlopen(req)</span><br><span class="line"><span class="comment"># 3、提取响应内容</span></span><br><span class="line">html = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(html)</span><br></pre></td></tr></table></figure>

<p>程序运行后，输出的请求头信息如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">F:\PyCharmProjects\venv\Scripts\python.exe F:/PyCharmProjects/Practise_Example/test.py</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;identity&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Python-urllib/3.9&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-633ed52a-287adc06553c454d579de238&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;183.200.174.154&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;http://httpbin.org/get&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从输出结果可以看出，User-Agent 竟然是 Python-urllib&#x2F;3.9，这显然是爬虫程序访问网站。因此就需要重构 User-Agent，将其伪装成“浏览器”访问网站。</p>
<p>注意：<a target="_blank" rel="noopener" href="http://httpbin.org/">httpbin.org </a>这个网站能测试 HTTP 请求和响应的各种信息，比如 cookie、IP、headers 和登录验证等，且支持 GET、POST 等多种方法，对 Web 开发和测试很有帮助。</p>
<h2 id="2-重构爬虫UA信息"><a href="#2-重构爬虫UA信息" class="headerlink" title="2)重构爬虫UA信息"></a>2)重构爬虫UA信息</h2><p>通过<a target="_blank" rel="noopener" href="https://useragent.buyaocha.com/">在线识别工具</a>，可以查看本机的浏览器版本以及 UA 信息</p>
<p>下面使用<code>urllib.request.Request()</code>方法重构 User-Agent 信息，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构请求头，伪装成 Mac火狐浏览器访问，可以使用上表中任意浏览器的UA信息</span></span><br><span class="line">my_headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:65.0) Gecko/20100101 Firefox/65.0&#x27;</span>&#125;</span><br><span class="line">url = <span class="string">&#x27;http://httpbin.org/get&#x27;</span></span><br><span class="line"><span class="comment"># 1、创建请求对象，包装ua信息</span></span><br><span class="line">req = urllib.request.Request(url=url, headers=my_headers)</span><br><span class="line"><span class="comment"># 2、向网站发送get请求</span></span><br><span class="line">response = urllib.request.urlopen(req)</span><br><span class="line"><span class="comment"># 3、提取响应内容</span></span><br><span class="line">html = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(html)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>程序的运行结果，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">F:\PyCharmProjects\venv\Scripts\python.exe F:/PyCharmProjects/Practise_Example/test.py</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Accept-Encoding&quot;</span>: <span class="string">&quot;identity&quot;</span>, </span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span>, </span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:65.0) Gecko/20100101 Firefox/65.0&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-633ed58d-6a4ab11747ed04a10e28b758&quot;</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;183.200.174.154&quot;</span>, </span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;http://httpbin.org/get&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码重构了 User-Agent 字符串信息，这样就解决了网站通过识别 User-Agent 来封杀爬虫程序的问题。当然这只是应对反爬策略的第一步。重构 UA 也可以通过其他模块实现，比如 requests 模块。</p>
<h1 id="三、构建User-Agnet代理池"><a href="#三、构建User-Agnet代理池" class="headerlink" title="三、构建User-Agnet代理池"></a>三、构建User-Agnet代理池</h1><p>在编写爬虫程序时，一般都会构建一个 User-Agent （用户代理）池，就是把多个浏览器的 UA 信息放进列表中，然后再从中随机选择。构建用户代理池，能够避免总是使用一个 UA 来访问网站，因为短时间内总使用一个 UA 高频率访问的网站，可能会引起网站的警觉，从而封杀掉 IP。</p>
<h2 id="1-自定义UA代理池"><a href="#1-自定义UA代理池" class="headerlink" title="1)自定义UA代理池"></a>1)自定义UA代理池</h2><p>构建代理池的方法也非常简单，在您的 Pycharm 工作目录中定义一个 ua_info.py 文件，并将以下 UA 信息以列表的形式粘贴到该文件中，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ua_list = [</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent:Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27; Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27; Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>经过上述操作，用户代理池就构建成功。</p>
<h2 id="2-模块随机获取UA"><a href="#2-模块随机获取UA" class="headerlink" title="2)模块随机获取UA"></a>2)模块随机获取UA</h2><p>您也可以使用专门第三方的模块来随机获取浏览器 UA 信息，不过该模块需要单独安装，安装方式如下：</p>
<p>pip install fake-useragent</p>
<p>下载安装成功后，演示如下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fake_useragent <span class="keyword">import</span> UserAgent</span><br><span class="line"></span><br><span class="line">ua = UserAgent()</span><br><span class="line"><span class="built_in">print</span>(ua.ie)</span><br><span class="line"><span class="built_in">print</span>(ua.ie)</span><br><span class="line"><span class="built_in">print</span>(ua.chrome)</span><br><span class="line"><span class="built_in">print</span>(ua.chrome)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机获取ie的ua信息</span></span><br><span class="line">Mozilla/<span class="number">5.0</span> (compatible; MSIE <span class="number">8.0</span>; Windows NT <span class="number">5.1</span>; Trident/<span class="number">4.0</span>; InfoPath<span class="number">.2</span>; SLCC1; .NET CLR <span class="number">3.0</span><span class="number">.4506</span><span class="number">.2152</span>; .NET CLR <span class="number">3.5</span><span class="number">.30729</span>; .NET CLR <span class="number">2.0</span><span class="number">.50727</span>)</span><br><span class="line">Mozilla/<span class="number">5.0</span> (compatible; MSIE <span class="number">10.0</span>; Windows NT <span class="number">6.1</span>; Trident/<span class="number">5.0</span>)</span><br><span class="line"><span class="comment">#随机获取火狐的ua信息</span></span><br><span class="line">Mozilla/<span class="number">5.0</span> (Windows NT <span class="number">4.0</span>; WOW64) AppleWebKit/<span class="number">537.36</span> (KHTML, like Gecko) Chrome/<span class="number">37.0</span><span class="number">.2049</span><span class="number">.0</span> Safari/<span class="number">537.36</span></span><br><span class="line">Mozilla/<span class="number">5.0</span> (Windows NT <span class="number">6.2</span>; WOW64) AppleWebKit/<span class="number">537.36</span> (KHTML, like Gecko) Chrome/<span class="number">29.0</span><span class="number">.1547</span><span class="number">.2</span> Safari/<span class="number">537.36</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-URL编码-x2F-解码详解"><a href="#3-URL编码-x2F-解码详解" class="headerlink" title="3)URL编码&#x2F;解码详解"></a>3)URL编码&#x2F;解码详解</h2><p>当 URL 路径或者查询参数中，带有中文或者特殊字符的时候，就需要对 URL 进行编码（采用十六进制编码格式）。URL 编码的原则是使用安全字符去表示那些不安全的字符。</p>
<p>安全字符，指的是没有特殊用途或者特殊意义的字符。</p>
<h3 id="3-1-URL基本组成"><a href="#3-1-URL基本组成" class="headerlink" title="3.1 URL基本组成"></a>3.1 URL基本组成</h3><p>URL 是由一些简单的组件构成，比如协议、域名、端口号、路径和查询字符串等，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.biancheng.net/index?param=<span class="number">10</span></span><br></pre></td></tr></table></figure>

<p>路径和查询字符串之间使用问号<code>?</code>隔开。上述示例的域名为 <a target="_blank" rel="noopener" href="http://www.biancheng.net,路径为/">www.biancheng.net，路径为</a> index，查询字符串为 param&#x3D;1。</p>
<p>URL 中规定了一些具有特殊意义的字符，常被用来分隔两个不同的 URL 组件，这些字符被称为<strong>保留字符</strong>。例如：</p>
<ul>
<li>冒号：用于分隔协议和主机组件，斜杠用于分隔主机和路径</li>
<li><code>?</code>：用于分隔路径和查询参数等。</li>
<li><code>=</code>用于表示查询参数中的键值对。</li>
<li><code>&amp;</code>符号用于分隔查询多个键值对。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">其余常用的保留字符有：/ . ... <span class="comment"># @ $ + ; %</span></span><br></pre></td></tr></table></figure>

<h3 id="3-2-哪些字符需要编码"><a href="#3-2-哪些字符需要编码" class="headerlink" title="3.2 哪些字符需要编码"></a>3.2 哪些字符需要编码</h3><p>URL 之所以需要编码，是因为 URL 中的某些字符会引起歧义，比如 URL 查询参数中包含了”&amp;”或者”%”就会造成服务器解析错误；再比如，URL 的编码格式采用的是 ASCII 码而非 Unicode 格式，这表明 URL 中不允许包含任何非 ASCII 字符（比如中文），否则就会造成 URL 解析错误。</p>
<p>URL 编码协议规定（RFC3986 协议）：URL 中只允许使用 ASCII 字符集可以显示的字符，比如英文字母、数字、和<code>- _ . ~ ! *</code>这 6 个<strong>特殊字符</strong>。当在 URL 中使用不属于 ASCII 字符集的字符时，就要使用特殊的符号对该字符进行编码，比如空格需要用<code>%20</code>来表示。</p>
<p>除了无法显示的字符需要编码外，还需要对 URL 中的部分<strong>保留字符</strong>和<strong>不安全字符</strong>进行编码。下面列举了部分不安全字符：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ ] &lt; &gt; <span class="string">&quot; &quot;</span>  &#123; &#125; | \ ^ * · ‘ ’ 等</span><br></pre></td></tr></table></figure>

<p>下面示例，查询字符串中包含一些特殊字符，这些特殊字符不需要编码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.biancheng.net/index?param=<span class="number">10</span>!*&amp;param1=<span class="number">20</span>!-~_</span><br></pre></td></tr></table></figure>

<p>下表对 URL 中部分保留字符和不安全字符进行了说明：</p>
<p><img src="/images/loading/loading.gif" data-original="image-20221006220332348.png" alt="image-20221006220332348"></p>
<p>下面简单总结一下，哪些字符需要编码，分为以下三种情况：</p>
<ul>
<li>ASCII 表中没有对应的可显示字符，例如，汉字。</li>
<li>不安全字符，包括：# ”% &lt;&gt; [] {} | \ ^ &#96; 。</li>
<li>部分保留字符，即 &amp; &#x2F; : ; &#x3D; ? @ 。</li>
</ul>
<h3 id="3-3-Python实现编码与解码"><a href="#3-3-Python实现编码与解码" class="headerlink" title="3.3 Python实现编码与解码"></a>3.3 Python实现编码与解码</h3><p>Python 的标准库<code>urllib.parse</code>模块中提供了用来编码和解码的方法，分别是 urlencode() 与 unquote() 方法。</p>
<p><img src="/images/loading/loading.gif" data-original="image-20221006220548511.png" alt="image-20221006220548511"></p>
<h4 id="1-编码urlencode"><a href="#1-编码urlencode" class="headerlink" title="1) 编码urlencode()"></a>1) 编码urlencode()</h4><p>下面以百度搜索为例进行讲解。首先打开百度首页，在搜索框中输入“爬虫”，然后点击“百度一下”。当搜索结果显示后，此时地址栏的 URL 信息，如下所示：</p>
<p><strong>注：</strong>1.<strong>%E7%88%AC%E8%99%AB</strong>&lt;&#x3D;&gt;<strong>爬虫</strong> 	2.<strong>word</strong>&lt;&#x3D;&gt;<strong>wd</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.baidu.com/s?word=%E7%88%AC%E8%99%AB&amp;tn=68018901_10_oem_dg">https://www.baidu.com/s?word=%E7%88%AC%E8%99%AB&amp;tn=68018901_10_oem_dg</a></p>
<p>第一个查询字符串就是“wd&#x3D;爬虫”，其中 wd 表示查询字符串的键，而“爬虫”则代表您输入的值。</p>
<p>在网页地址栏中删除多余的查询字符串，最后显示的 URL 如下所示：</p>
<p><a target="_blank" rel="noopener" href="https://www.baidu.com/s?word=%E7%88%AC%E8%99%AB">https://www.baidu.com/s?word=爬虫</a></p>
<p>使用搜索修改后的 URL 进行搜索，依然会得到相同页面。因此可知“wd”参数是百度搜索的关键查询参数。下面编写爬虫程序对 “wd&#x3D;爬虫”进行编码，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入parse模块</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建查询字符串字典</span></span><br><span class="line">query_string = &#123;<span class="string">&#x27;wd&#x27;</span>: <span class="string">&#x27;爬虫&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 调用parse模块的urlencode()进行编码</span></span><br><span class="line">result = parse.urlencode(query_string)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># 使用format函数格式化字符串，拼接url地址</span></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(result)</span><br><span class="line"><span class="built_in">print</span>(url)</span><br></pre></td></tr></table></figure>

<p>输出结果，如下所示：</p>
<p>​	wd&#x3D;%E7%88%AC%E8%99%AB<br>​        	<a target="_blank" rel="noopener" href="http://www.baidu.com/s?wd=%E7%88%AC%E8%99%AB">http://www.baidu.com/s?wd=%E7%88%AC%E8%99%AB</a></p>
<p>编码后的 URL 地址依然可以通过地网页址栏实现搜索功能。</p>
<p>除了使用 urlencode() 方法之外，也可以使用 quote(string) 方法实现编码，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://baidu.com/s?wd=&#123;&#125;&#x27;</span></span><br><span class="line">wd = <span class="built_in">input</span>(<span class="string">&#x27;please input what you want to search:\n&#x27;</span>)</span><br><span class="line">query_str = parse.quote(wd)</span><br><span class="line"><span class="built_in">print</span>(url.<span class="built_in">format</span>(query_str))</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">please <span class="built_in">input</span> what you want to search:</span><br><span class="line">Python是世界上最好的语言</span><br><span class="line">http://baidu.com/s?wd=Python%E6%<span class="number">98</span>%AF%E4%B8%<span class="number">96</span>%E7%<span class="number">95</span>%8C%E4%B8%8A%E6%9C%<span class="number">80</span>%E5%A5%BD%E7%9A%<span class="number">84</span>%E8%AF%AD%E8%A8%<span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>注意：quote() 只能对字符串编码，而 urlencode() 可以直接对查询字符串字典进行编码。因此在定义 URL 时，需要注意两者之间的差异。方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># urllib.parse</span></span><br><span class="line">urllib.parse.urlencode(&#123;<span class="string">&#x27;key&#x27;</span>:<span class="string">&#x27;value&#x27;</span>&#125;) <span class="comment">#字典</span></span><br><span class="line">urllib.parse.quote(string) <span class="comment">#字符串</span></span><br></pre></td></tr></table></figure>

<h4 id="2-解码unquote-string"><a href="#2-解码unquote-string" class="headerlink" title="2) 解码unquote(string)"></a>2) 解码unquote(string)</h4><p>解码是对编码后的 URL 进行还原的一种操作，示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码后的url</span></span><br><span class="line"><span class="built_in">str</span> = <span class="string">&#x27;%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6&#x27;</span></span><br><span class="line"><span class="comment"># url再封装</span></span><br><span class="line">str2 = parse.quote(<span class="built_in">str</span>)</span><br><span class="line"><span class="comment"># url还原</span></span><br><span class="line">result = parse.unquote(<span class="built_in">str</span>)</span><br><span class="line"><span class="comment"># 封装后的url还原</span></span><br><span class="line">result2 = parse.unquote(str2)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="built_in">print</span>(result2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<p>​	清华大学<br>​			%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6</p>
<h4 id="3-URL地址拼接方式"><a href="#3-URL地址拼接方式" class="headerlink" title="3) URL地址拼接方式"></a>3) URL地址拼接方式</h4><p>最后，给大家介绍三种拼接 URL 地址的方法。除了使用 format() 函数外，还可以使用字符串相加，以及字符串占位符，总结如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.字符串相加</span></span><br><span class="line">baseUrl = <span class="string">&#x27;http://www.baidu.com/s?&#x27;</span></span><br><span class="line">params = <span class="string">&#x27;wd=%E7%BC%96%E7%A8%8B&#x27;</span></span><br><span class="line">url = baseUrl + params</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;字符串相加：&#x27;</span> + url)</span><br><span class="line"><span class="comment"># 2.字符串格式化</span></span><br><span class="line">url1 = <span class="string">&#x27;http://www.baidu.com/s?%s&#x27;</span> % params</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;字符串格式化：&#x27;</span> + url1)</span><br><span class="line"><span class="comment"># 3.format()方法</span></span><br><span class="line">url2 = <span class="string">&#x27;http://www.baidu.com/s?&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(params)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;format()方法：&#x27;</span> + url2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<p>​	字符串相加：<a target="_blank" rel="noopener" href="http://www.baidu.com/s?wd=%E7%BC%96%E7%A8%8B">http://www.baidu.com/s?wd=%E7%BC%96%E7%A8%8B</a><br>​			字符串格式化：<a target="_blank" rel="noopener" href="http://www.baidu.com/s?wd=%E7%BC%96%E7%A8%8B">http://www.baidu.com/s?wd=%E7%BC%96%E7%A8%8B</a><br>​		 	format()方法：<a target="_blank" rel="noopener" href="http://www.baidu.com/s?wd=%E7%BC%96%E7%A8%8B">http://www.baidu.com/s?wd=%E7%BC%96%E7%A8%8B</a></p>
<h1 id="四、【实例】Python爬虫抓取网页"><a href="#四、【实例】Python爬虫抓取网页" class="headerlink" title="四、【实例】Python爬虫抓取网页"></a>四、【实例】Python爬虫抓取网页</h1><p>本节讲解第一个 Python 爬虫实战案例：抓取您想要的网页，并将其保存至本地计算机。</p>
<p>首先我们对要编写的爬虫程序进行简单地分析，该程序可分为以下三个部分：</p>
<ul>
<li><p>拼接 url 地址</p>
</li>
<li><p>发送请求</p>
</li>
<li><p>将文件保存至本地</p>
<p>​</p>
</li>
</ul>
<p>		</p>
<p>明确逻辑后，我们就可以正式编写爬虫程序了。</p>
<h2 id="1-导入所需模块"><a href="#1-导入所需模块" class="headerlink" title="1)导入所需模块"></a>1)导入所需模块</h2><p>本节内容使用 urllib 库来编写爬虫，下面导入程序所用模块：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br></pre></td></tr></table></figure>

<h2 id="2-拼接URL地址"><a href="#2-拼接URL地址" class="headerlink" title="2)拼接URL地址"></a>2)拼接URL地址</h2><p>定义 URL 变量，拼接 url 地址。代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?wd=&#123;&#125;&#x27;</span></span><br><span class="line"><span class="comment">#想要搜索的内容</span></span><br><span class="line">word = <span class="built_in">input</span>(<span class="string">&#x27;请输入搜索内容:&#x27;</span>)</span><br><span class="line">params = parse.quote(word)</span><br><span class="line">full_url = url.<span class="built_in">format</span>(params)</span><br></pre></td></tr></table></figure>

<h2 id="3-向URL发送请求"><a href="#3-向URL发送请求" class="headerlink" title="3)向URL发送请求"></a>3)向URL发送请求</h2><p>发送请求主要分为以下几个步骤：</p>
<ul>
<li>创建请求对象-Request</li>
<li>获取响应对象-urlopen</li>
<li>获取响应内容-read</li>
</ul>
<p>代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重构请求头</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0&#x27;</span>&#125;</span><br><span class="line"><span class="comment">#创建请求对应</span></span><br><span class="line">req = request.Request(url=full_url,headers=headers)</span><br><span class="line"><span class="comment">#获取响应对象</span></span><br><span class="line">res = request.urlopen(req)</span><br><span class="line"><span class="comment">#获取响应内容</span></span><br><span class="line">html = res.read().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="4-保存为本地文件"><a href="#4-保存为本地文件" class="headerlink" title="4)保存为本地文件"></a>4)保存为本地文件</h2><p>把爬取的照片保存至本地，此处需要使用 Python 编程的文件 IO 操作，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filename = word + <span class="string">&#x27;.html&#x27;</span><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:    f.write(html)</span><br></pre></td></tr></table></figure>

<p>完整程序如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line"><span class="comment"># 1.拼url地址</span></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?wd=&#123;&#125;&#x27;</span></span><br><span class="line">word = <span class="built_in">input</span>(<span class="string">&#x27;请输入搜索内容:&#x27;</span>)</span><br><span class="line">params = parse.quote(word)</span><br><span class="line">full_url = url.<span class="built_in">format</span>(params)</span><br><span class="line"><span class="comment"># 2.发请求保存到本地</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0&#x27;</span>&#125;</span><br><span class="line">req = request.Request(url=full_url,headers=headers)</span><br><span class="line">res = request.urlopen(req)</span><br><span class="line">html = res.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 3.保存文件至当前目录</span></span><br><span class="line">filename = word + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(html)</span><br></pre></td></tr></table></figure>

<p>尝试运行程序，并输入编程帮，确认搜索，然后您会在 Pycharm 当前的工作目录中找到“编程帮.html”文件。</p>
<h2 id="5-函数式编程修改程序"><a href="#5-函数式编程修改程序" class="headerlink" title="5)函数式编程修改程序"></a>5)函数式编程修改程序</h2><p>Python 函数式编程可以让程序的思路更加清晰、易懂。接下来，使用函数编程的思想更改上面代码。</p>
<p>定义相应的函数，通过调用函数来执行爬虫程序。修改后的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拼接url</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_url</span>(<span class="params">word</span>):</span><br><span class="line">    url = <span class="string">&#x27;http://www.baidu.com/s?&#123;&#125;&#x27;</span></span><br><span class="line">    params = parse.urlencode(&#123;<span class="string">&#x27;wd&#x27;</span>: word&#125;)</span><br><span class="line">    url = url.<span class="built_in">format</span>(params)</span><br><span class="line">    <span class="keyword">return</span> url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送请求，保存本地文件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">request_url</span>(<span class="params">url, filename</span>):</span><br><span class="line">    <span class="comment"># 伪装头，发送请求并打开url</span></span><br><span class="line">    headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0&#x27;</span>&#125;</span><br><span class="line">    req = request.Request(url=url, headers=headers)</span><br><span class="line">    res = request.urlopen(req)</span><br><span class="line">    html = res.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="comment"># 保存文件</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(html)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    word = <span class="built_in">input</span>(<span class="string">&#x27;请输入搜索内容：&#x27;</span>)</span><br><span class="line">    url = get_url(word)</span><br><span class="line">    filename = word + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line">    request_url(url, filename)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>除了使用函数式编程外，您也可以使用面向对象的编程方法（本教程主要以该方法），在后续内容中会做相应介绍。</p>
<h1 id="五、【实例】Python爬虫抓取百度贴吧数据"><a href="#五、【实例】Python爬虫抓取百度贴吧数据" class="headerlink" title="五、【实例】Python爬虫抓取百度贴吧数据"></a>五、【实例】Python爬虫抓取百度贴吧数据</h1><p>本节继续讲解 Python 爬虫实战案例：抓取百度贴吧（<a target="_blank" rel="noopener" href="https://tieba.baidu.com/%EF%BC%89%E9%A1%B5%E9%9D%A2%EF%BC%8C%E6%AF%94%E5%A6%82">https://tieba.baidu.com/）页面，比如</a> Python爬虫吧、编程吧，只抓取贴吧的前 5 个页面即可。本节我们将使用面向对象的编程方法来编写程序。</p>
<h2 id="1-判断页面类型"><a href="#1-判断页面类型" class="headerlink" title="1)判断页面类型"></a>1)判断页面类型</h2><p>通过简单的分析可以得知，待抓取的百度贴吧页面属于静态网页，分析方法非常简单：打开百度贴吧，搜索“Python爬虫”，在出现的页面中复制任意一段信息，比如“爬虫需要 http 代理的原因”，然后点击右键选择查看源码，并使用 Ctrl+F 快捷键在源码页面搜索刚刚复制的数据，如下所示：</p>
<p><img src="/images/loading/loading.gif" data-original="9-210Q9112J2329.png" alt="img"></p>
<p>由上图可知，页面内的所有信息都包含在源码页中，数据并不需要从数据库另行加载，因此该页面属于静态页面。</p>
<h2 id="2-寻找URL变化规律"><a href="#2-寻找URL变化规律" class="headerlink" title="2)寻找URL变化规律"></a>2)寻找URL变化规律</h2><p>接下来寻找要爬取页面的 URL 规律，搜索“Python爬虫”后，此时贴吧第一页的的 url 如下所示：</p>
<p><a target="_blank" rel="noopener" href="https://tieba.baidu.com/f?ie=utf-8&amp;kw=python%E7%88%AC%E8%99%AB&amp;fr=search">https://tieba.baidu.com/f?ie=utf-8&amp;kw=python爬虫&amp;fr=search</a></p>
<p>点击第二页，其 url 信息如下：</p>
<p><a target="_blank" rel="noopener" href="https://tieba.baidu.com/f?kw=python%E7%88%AC%E8%99%AB&amp;ie=utf-8&amp;pn=50">https://tieba.baidu.com/f?kw=python爬虫&amp;ie=utf-8&amp;pn=50</a></p>
<p>点击第三页，url 信息如下：</p>
<p><a target="_blank" rel="noopener" href="https://tieba.baidu.com/f?kw=python%E7%88%AC%E8%99%AB&amp;ie=utf-8&amp;pn=100">https://tieba.baidu.com/f?kw=python爬虫&amp;ie=utf-8&amp;pn=100</a></p>
<p>重新点击第一页，url 信息如下：</p>
<p><a target="_blank" rel="noopener" href="https://tieba.baidu.com/f?kw=python%E7%88%AC%E8%99%AB&amp;ie=utf-8&amp;pn=0">https://tieba.baidu.com/f?kw=python爬虫&amp;ie=utf-8&amp;pn=0</a></p>
<p>如果还不确定，您可以继续多浏览几页。最后您发现 url 具有两个查询参数，分别是 kw 和 pn，并且 pn 参数具有规律性，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">第n页：pn=(n-<span class="number">1</span>)*<span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#参数params</span></span><br><span class="line">pn=(page-<span class="number">1</span>)*<span class="number">50</span></span><br><span class="line">params=&#123;</span><br><span class="line">         <span class="string">&#x27;kw&#x27;</span>:name,</span><br><span class="line">         <span class="string">&#x27;pn&#x27;</span>:<span class="built_in">str</span>(pn)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>url 地址可以简写为：</p>
<p><a target="_blank" rel="noopener" href="https://tieba.baidu.com/f?kw=python%E7%88%AC%E8%99%AB&amp;pn=450">https://tieba.baidu.com/f?kw=python爬虫&amp;pn=450</a></p>
<h2 id="3-编写爬虫程序"><a href="#3-编写爬虫程序" class="headerlink" title="3)编写爬虫程序"></a>3)编写爬虫程序</h2><p><strong>UA池(ua_info.py)</strong></p>
<p>​	导入方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> ua_info <span class="keyword">import</span> ua_list </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ua_list = [</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;User-Agent:Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27; Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27; Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>下面以类的形式编写爬虫程序，并在类下编写不同的功能函数，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> ua_info <span class="keyword">import</span> ua_list  <span class="comment"># 导入自己写好的UA池</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬虫类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiDuTieBa_Spider</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 定义成员变量url</span></span><br><span class="line">        self.url = <span class="string">&#x27;http://tieba.baidu.com/f?&#123;&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 请求</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_html</span>(<span class="params">self, url</span>):</span><br><span class="line">        <span class="comment"># 从ua池中随机选择一个ua</span></span><br><span class="line">        req = request.Request(url=url, headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>: random.choice(ua_list)&#125;)</span><br><span class="line">        res = request.urlopen(req)</span><br><span class="line">        <span class="comment"># windows会存在乱码问题，需要使用 gbk解码，并使用ignore忽略不能处理的字节</span></span><br><span class="line">        <span class="comment"># linux不会存在上述问题，可以直接使用decode(&#x27;utf-8&#x27;)解码</span></span><br><span class="line">        html = res.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_html</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_html</span>(<span class="params">self, filename, html</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(html)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 入口</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        name = <span class="built_in">input</span>(<span class="string">&#x27;输入贴吧名：&#x27;</span>)</span><br><span class="line">        begin = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;输入起始页：&#x27;</span>))</span><br><span class="line">        end = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;输入结束页：&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># +1保证可以取到整数</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(begin, end + <span class="number">1</span>):</span><br><span class="line">            pn = (page - <span class="number">1</span>) * <span class="number">50</span></span><br><span class="line">            params = &#123;<span class="string">&#x27;kw&#x27;</span>: name,</span><br><span class="line">                      <span class="string">&#x27;pn&#x27;</span>: <span class="built_in">str</span>(pn)&#125;</span><br><span class="line">            <span class="comment"># 拼接url</span></span><br><span class="line">            params = parse.urlencode(params)  <span class="comment"># 将参数编码</span></span><br><span class="line">            url = self.url.<span class="built_in">format</span>(params)</span><br><span class="line">            <span class="built_in">print</span>(url)</span><br><span class="line">            <span class="comment"># 发送请求,返回html</span></span><br><span class="line">            html = self.get_html(url)</span><br><span class="line">            <span class="comment"># 定义路径</span></span><br><span class="line">            filename = <span class="string">&#x27;&#123;&#125;吧-第&#123;&#125;页.html&#x27;</span>.<span class="built_in">format</span>(name, page)</span><br><span class="line">            self.save_html(filename, html)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 提示</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;第&#123;&#125;页爬取成功！&#x27;</span>.<span class="built_in">format</span>(page))</span><br><span class="line">            <span class="comment"># 爬虫休眠</span></span><br><span class="line">            time.sleep(random.randint(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="comment"># 实例化爬虫类的对象spider</span></span><br><span class="line">    spider = BaiDuTieBa_Spider()</span><br><span class="line">    spider.run()</span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="comment"># 获取程序执行时间</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;执行时间：&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(end - start))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>程序执行后，爬取的文件将会保存至 Pycharm 当前工作目录，输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">输入贴吧名：c++</span><br><span class="line">输入起始页：1</span><br><span class="line">输入结束页：1</span><br><span class="line">http://tieba.baidu.com/f?kw=c%2B%2B&amp;pn=0</span><br><span class="line">第1页爬取成功！</span><br><span class="line">执行时间：15.13</span><br></pre></td></tr></table></figure>

<p>以面向对象方法编写爬虫程序时，思路简单、逻辑清楚，非常容易理解，上述代码主要包含了四个功能函数，它们分别负责了不同的功能，总结如下：</p>
<h4 id="1-请求函数"><a href="#1-请求函数" class="headerlink" title="1) 请求函数"></a>1) 请求函数</h4><p>请求函数最终的结果是返回一个 HTML 对象，以方便后续的函数调用它。 </p>
<h4 id="2-解析函数"><a href="#2-解析函数" class="headerlink" title="2) 解析函数"></a>2) 解析函数</h4><p>解析函数用来解析 HTML 页面，常用的解析模块有正则解析模块、bs4 解析模块。通过分析页面，提取出所需的数据，在后续内容会做详细介绍。</p>
<h4 id="3-保存数据函数"><a href="#3-保存数据函数" class="headerlink" title="3) 保存数据函数"></a>3) 保存数据函数</h4><p>该函数负责将抓取下来的数据保至数据库中，比如 MySQL、MongoDB 等，或者将其保存为文件格式，比如 csv、txt、excel 等。</p>
<h4 id="4-入口函数"><a href="#4-入口函数" class="headerlink" title="4) 入口函数"></a>4) 入口函数</h4><p>入口函数充当整个爬虫程序的桥梁，通过调用不同的功能函数，实现数据的最终抓取。入口函数的主要任务是组织数据，比如要搜索的贴吧名、编码 url 参数、拼接 url 地址、定义文件保存路径。</p>
<h2 id="4-爬虫程序结构"><a href="#4-爬虫程序结构" class="headerlink" title="4)爬虫程序结构"></a>4)爬虫程序结构</h2><p>用面向对象的方法编写爬虫程序时，逻辑结构较为固定，总结如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 程序结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">xxxSpider</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 定义常用变量,比如url或计数变量等</span></span><br><span class="line">       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_html</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 获取响应内容函数,使用随机User-Agent</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_html</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 使用正则表达式来解析页面，提取数据</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">write_html</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 将提取的数据按要求保存，csv、MySQL数据库等</span></span><br><span class="line">       </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 主函数，用来控制整体逻辑</span></span><br><span class="line">       </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 程序开始运行时间</span></span><br><span class="line">    spider = xxxSpider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure>

<p>注意：掌握以上编程逻辑有助于您后续的学习。</p>
<h2 id="5-爬虫程序随机休眠"><a href="#5-爬虫程序随机休眠" class="headerlink" title="5)爬虫程序随机休眠"></a>5)爬虫程序随机休眠</h2><p>在入口函数代码中，包含了以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#每爬取一个页面随机休眠1-2秒钟的时间</span></span><br><span class="line">time.sleep(random.randint(<span class="number">1</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p>爬虫程序访问网站会非常快，这与正常人类的点击行为非常不符。因此，通过随机休眠可以使爬虫程序模仿成人类的样子点击网站，从而让网站不易察觉是爬虫访问网站，但这样做的代价就是影响程序的执行效率。</p>
<p>聚焦爬虫是一种执行效率较低的程序，提升其性能，是业界一直关注的问题，由此也诞生了效率较高的 Python 爬虫框架 Scrapy。</p>
<h1 id="六、正则表达式基本语法"><a href="#六、正则表达式基本语法" class="headerlink" title="六、正则表达式基本语法"></a>六、正则表达式基本语法</h1><p>正则表达式(regular expression)是一种字符串匹配模式或者规则，它可以用来检索、替换那些符合特定规则的文本。正则表达式几乎适用于所有编程语言，无论是前端语言 JavaScript，还是诸如许多后端语言，比如 Python、Java、C# 等，这些语言都提供了相应的函数、模块来支持正则表达式，比如 Python 的 re 模块就提供了正则表达式的常用方法。</p>
<p>在使用 Python 编写爬虫的过程中，re 模块通常做为一种解析方法来使用。通过审查网页元素来获取网页的大体结构，然后使用解析模块来提取你想要的网页信息，最终实现数据的抓取。本节对正则表达式基本语法做简单讲解。</p>
<p>注意：学习本节知识之前，您应该已经掌握了正则表达式的使用方法。</p>
<h2 id="1-正则表达式元字符"><a href="#1-正则表达式元字符" class="headerlink" title="1)正则表达式元字符"></a>1)正则表达式元字符</h2><p>下表列出了常用的正则表达式元字符：</p>
<h4 id="1-元字符"><a href="#1-元字符" class="headerlink" title="1) 元字符"></a>1) 元字符</h4><table>
<thead>
<tr>
<th>元字符</th>
<th align="left">匹配内容</th>
</tr>
</thead>
<tbody><tr>
<td>.</td>
<td align="left">匹配除换行符以外的任意字符</td>
</tr>
<tr>
<td>\w</td>
<td align="left">匹配所有普通字符(数字、字母或下划线)</td>
</tr>
<tr>
<td>\s</td>
<td align="left">匹配任意的空白符</td>
</tr>
<tr>
<td>\d</td>
<td align="left">匹配数字</td>
</tr>
<tr>
<td>\n</td>
<td align="left">匹配一个换行符</td>
</tr>
<tr>
<td>\t</td>
<td align="left">匹配一个制表符</td>
</tr>
<tr>
<td>\b</td>
<td align="left">匹配一个单词的结尾</td>
</tr>
<tr>
<td>^</td>
<td align="left">匹配字符串的开始位置</td>
</tr>
<tr>
<td>$</td>
<td align="left">匹配字符串的结尾位置</td>
</tr>
<tr>
<td>\W</td>
<td align="left">匹配非字母或数字或下划线</td>
</tr>
<tr>
<td>\D</td>
<td align="left">匹配非数字</td>
</tr>
<tr>
<td>\S</td>
<td align="left">匹配非空白符</td>
</tr>
<tr>
<td>a|b</td>
<td align="left">匹配字符 a 或字符 b</td>
</tr>
<tr>
<td>()</td>
<td align="left">正则表达式分组所用符号，匹配括号内的表达式，表示一个组。</td>
</tr>
<tr>
<td>[…]</td>
<td align="left">匹配字符组中的字符</td>
</tr>
<tr>
<td>[^…]</td>
<td align="left">匹配除了字符组中字符的所有字符</td>
</tr>
</tbody></table>
<h4 id="2-量词"><a href="#2-量词" class="headerlink" title="2) 量词"></a>2) 量词</h4><table>
<thead>
<tr>
<th>量词</th>
<th>用法说明</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>重复零次或者更多次</td>
</tr>
<tr>
<td>+</td>
<td>重复一次或者更多次</td>
</tr>
<tr>
<td>？</td>
<td>重复0次或者一次</td>
</tr>
<tr>
<td>{n}</td>
<td>重复n次</td>
</tr>
<tr>
<td>{n,}</td>
<td>重复n次或者更多次</td>
</tr>
<tr>
<td>{n,m}</td>
<td>重复n到m次</td>
</tr>
</tbody></table>
<h4 id="3-字符组"><a href="#3-字符组" class="headerlink" title="3) 字符组"></a>3) 字符组</h4><p>有时也会出现各种字符组成的字符组，这在正则表达式中使用<code>[]</code>表示，如下所示：</p>
<table>
<thead>
<tr>
<th>正则</th>
<th>待匹配字符</th>
<th>匹配结果</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>[0123456789]</td>
<td>8</td>
<td>True</td>
<td>在一个字符组里枚举所有字符，字符组里的任意一个字符 和”待匹配字符”相同都视为可以匹配。</td>
</tr>
<tr>
<td>[0123456789]</td>
<td>a</td>
<td>False</td>
<td>由于字符组中没有 “a” 字符，所以不能匹配。</td>
</tr>
<tr>
<td>[0-9]</td>
<td>7</td>
<td>True</td>
<td>也可以用-表示范围，[0-9] 就和 [0123456789] 是一个意思。</td>
</tr>
<tr>
<td>[a-z]</td>
<td>s</td>
<td>True</td>
<td>同样的如果要匹配所有的小写字母，直接用 [a-z] 就可以表示。</td>
</tr>
<tr>
<td>[A-Z]</td>
<td>B</td>
<td>True</td>
<td>[A-Z] 就表示所有的大写字母。</td>
</tr>
<tr>
<td>[0-9a-fA-F]</td>
<td>e</td>
<td>True</td>
<td>可以匹配数字，大小写形式的 a～f，用来验证十六进制字符。</td>
</tr>
</tbody></table>
<h2 id="2-贪婪模式非贪婪模式"><a href="#2-贪婪模式非贪婪模式" class="headerlink" title="2)贪婪模式非贪婪模式"></a>2)贪婪模式非贪婪模式</h2><p>正则表达式默认为贪婪匹配，也就是尽可能多的向后匹配字符，比如 {n,m} 表示匹配前面的内容出现 n 到 m 次（n 小于 m），在贪婪模式下，首先以匹配 m 次为目标，而在非贪婪模式是尽可能少的向后匹配内容，也就是说匹配 n 次即可。</p>
<p>贪婪模式转换为非贪婪模式的方法很简单，在元字符后添加“?”即可实现，如下所示：</p>
<table>
<thead>
<tr>
<th>元字符(贪婪模式)</th>
<th>非贪婪模式</th>
</tr>
</thead>
<tbody><tr>
<td>*</td>
<td>*?</td>
</tr>
<tr>
<td>+</td>
<td>+？</td>
</tr>
<tr>
<td>？</td>
<td>??</td>
</tr>
<tr>
<td>{n,m}</td>
<td>{n,m}？</td>
</tr>
</tbody></table>
<h2 id="3-正则表达式转义"><a href="#3-正则表达式转义" class="headerlink" title="3)正则表达式转义"></a>3)正则表达式转义</h2><p>如果使用正则表达式匹配特殊字符时，则需要在字符前加<code>\</code>表示转意。常见的特殊字符如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* + ? ^ $ [] () &#123;&#125; | \</span><br></pre></td></tr></table></figure>

<h1 id="七、Python-re模块用法详解"><a href="#七、Python-re模块用法详解" class="headerlink" title="七、Python re模块用法详解"></a>七、Python re模块用法详解</h1><p>在 Python 爬虫过程中，实现网页元素解析的方法有很多，正则解析只是其中之一，常见的还有 BeautifulSoup 和 lxml，它们都支持网页 HTML 元素的解析操作。本节重点讲解如何使用 re 正则解析模块实现网页信息的提取。</p>
<p>注意：在学习本节知识之前，您应该基本掌握了 Python re 模块的常用方法。</p>
<h2 id="1-re模块常用方法"><a href="#1-re模块常用方法" class="headerlink" title="1)re模块常用方法"></a>1)re模块常用方法</h2><h4 id="1-re-compile"><a href="#1-re-compile" class="headerlink" title="1) re.compile()"></a>1) re.compile()</h4><p>该方法用来<strong>生成</strong>正则表达式对象，其语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regex=re.compile(pattern,flags=0)</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>pattern：正则表达式对象。</li>
<li>flags：代表功能标志位，扩展正则表达式的匹配。</li>
</ul>
<h4 id="2-re-findall"><a href="#2-re-findall" class="headerlink" title="2) re.findall()"></a>2) re.findall()</h4><p>根据<strong>正则表达式匹配</strong>目标字符串内容。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(pattern,string,flags=0)</span><br></pre></td></tr></table></figure>

<p>该函数的返回值是匹配到的内容列表，如果正则表达式有子组，则只能获取到子组对应的内容。参数说明如下：</p>
<ul>
<li>pattern：正则表达式对象。</li>
<li>string：目标字符串</li>
<li>flags：代表功能标志位，扩展正则表达式的匹配。</li>
</ul>
<h4 id="3-regex-findall"><a href="#3-regex-findall" class="headerlink" title="3) regex.findall()"></a>3) regex.findall()</h4><p>该函数根据<strong>正则表达式对象匹配</strong>目标字符串内容。其语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regex.findall(string,pos,endpos)</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>string 目标字符串。</li>
<li>pos 截取目标字符串的开始匹配位置。</li>
<li>endpos 截取目标字符串的结束匹配位置。</li>
</ul>
<h4 id="4-re-split"><a href="#4-re-split" class="headerlink" title="4) re.split()"></a>4) re.split()</h4><p>该函数使用正则表达式匹配内容，<strong>切割</strong>目标字符串。返回值是切割后的内容列表。参数说明：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(pattern,string,flags = 0)</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>pattern：正则表达式。</li>
<li>string：目标字符串。</li>
<li>flags：功能标志位,扩展正则表达式的匹配。</li>
</ul>
<p>\5) re.sub<br>该函数使用一个字符串<strong>替换</strong>正则表达式匹配到的内容。返回值是替换后的字符串。其语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.sub(pattern,replace,string,max,flags = 0)</span><br></pre></td></tr></table></figure>

<p>其参数说明：</p>
<ul>
<li>pattern：正则表达式。</li>
<li>replace：替换的字符串。</li>
<li>string：目标字符串。</li>
<li>max：最多替换几处，默认替换全部，</li>
<li>flags：功能标志位,扩展正则表达式的匹配。</li>
</ul>
<h4 id="5-re-search"><a href="#5-re-search" class="headerlink" title="5) re.search()"></a>5) re.search()</h4><p>匹配目标字符串第一个符合的内容，<strong>返回</strong>值为<strong>匹配的对象</strong>。语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.search(pattern,string,flags=0)</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>pattern：正则表达式</li>
<li>string：目标字符串</li>
</ul>
<h2 id="2-flags功能标志位"><a href="#2-flags功能标志位" class="headerlink" title="2)flags功能标志位"></a>2)flags功能标志位</h2><p>功能标志位的作用是扩展正则表达的匹配功能。常用的 flag 如下所示：</p>
<table>
<thead>
<tr>
<th>缩写元字符</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>元字符只能匹配 ASCII码。</td>
</tr>
<tr>
<td>I</td>
<td>匹配忽略字母大小写。</td>
</tr>
<tr>
<td>S</td>
<td>使得<code>.</code>元字符可以匹配换行符。</td>
</tr>
<tr>
<td>M</td>
<td>使 ^ $ 可以匹配每一行的开头和结尾位置。</td>
</tr>
</tbody></table>
<p>注意：可以同时使用福多个功能标志位，比如 flags&#x3D;re.I|re.S。</p>
<p>下面使用贪婪和非贪婪两种模式来匹配 HTML 元素，分别，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">html = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;div&gt;&lt;p&gt;www.biancheng.net&lt;/p&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;div&gt;&lt;p&gt;编程帮&lt;/p&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># .S可以匹配到换行符</span></span><br><span class="line"><span class="comment"># 贪婪匹配</span></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;div&gt;&lt;p&gt;.*&lt;/p&gt;&lt;/div&gt;&#x27;</span>, re.S)  <span class="comment"># 创建正则表达式对象</span></span><br><span class="line">re_list = pattern.findall(html)  <span class="comment"># 通过正则表达式对象来匹配html元素，提取信息</span></span><br><span class="line"><span class="built_in">print</span>(re_list)</span><br><span class="line"><span class="comment"># 非贪婪匹配</span></span><br><span class="line">pattern2 = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;div&gt;&lt;p&gt;.*?&lt;/p&gt;&lt;/div&gt;&#x27;</span>, re.S)  <span class="comment"># 创建正则表达式对象</span></span><br><span class="line">re_list2 = pattern.findall(html)  <span class="comment"># 通过正则表达式对象来匹配html元素，提取信息</span></span><br><span class="line"><span class="built_in">print</span>(re_list2)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;&lt;div&gt;&lt;p&gt;www.biancheng.net&lt;/p&gt;&lt;/div&gt;\n&lt;div&gt;&lt;p&gt;编程帮&lt;/p&gt;&lt;/div&gt;&#x27;]</span><br><span class="line">[&#x27;&lt;div&gt;&lt;p&gt;www.biancheng.net&lt;/p&gt;&lt;/div&gt;\n&lt;div&gt;&lt;p&gt;编程帮&lt;/p&gt;&lt;/div&gt;&#x27;]</span><br></pre></td></tr></table></figure>

<p>从上述输出结果可以得出非贪婪模式比适合提取 HTML 信息。</p>
<h2 id="3-正则表达式分组"><a href="#3-正则表达式分组" class="headerlink" title="3)正则表达式分组"></a>3)正则表达式分组</h2><p>通过正则表达式分组可以从匹配的信息中提取出想要的信息。示例演示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则表达式分组</span></span><br><span class="line">website = <span class="string">&quot;编程帮 www.biancheng.net&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取所有信息</span></span><br><span class="line"><span class="comment"># | .    | 匹配除换行符以外的任意字符           |</span></span><br><span class="line"><span class="comment"># | \w   | 匹配所有普通字符(数字、字母或下划线) |</span></span><br><span class="line"><span class="comment"># | \s   | 匹配任意的空白符                     |</span></span><br><span class="line"><span class="comment"># 注意此时正则表达式的 &quot;.&quot; 需要转义因此使用 \.</span></span><br><span class="line">pattern_1 = re.<span class="built_in">compile</span>(<span class="string">&#x27;\w+\s+\w+\.\w+\.\w+&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(pattern_1.findall(website))</span><br><span class="line"><span class="comment"># 提取匹配信息的第一项</span></span><br><span class="line">pattern_2 = re.<span class="built_in">compile</span>(<span class="string">&#x27;(\w+)\s+\w+\.\w+\.\w+&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(pattern_2.findall(website))</span><br><span class="line"><span class="comment"># 有两个及以上的()则以元组形式显示</span></span><br><span class="line">pattern_3 = re.<span class="built_in">compile</span>(<span class="string">&#x27;(\w+)\s+(\w+\.\w+\.\w+)&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(pattern_3.findall(website))</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;编程帮 www.biancheng.net&#x27;]</span><br><span class="line">[&#x27;编程帮&#x27;]</span><br><span class="line">[(&#x27;编程帮&#x27;, &#x27;www.biancheng.net&#x27;)]</span><br></pre></td></tr></table></figure>

<p>正则表达式分组是提取信息的常用方式。当需要哪个特定信息的时候，就可以通过分组(也就是加括号)的方式获得。</p>
<h2 id="4-网页信息提取"><a href="#4-网页信息提取" class="headerlink" title="4)网页信息提取"></a>4)网页信息提取</h2><p>实战演练：从下面的 HTML 代码中使用 re 模块提取出两部影片的名称和主演信息。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">html = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;div class=&quot;movie-item-info&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;name&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a title=&quot;你好，李焕英&quot;&gt;你好，李焕英&lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;star&quot;&gt;</span></span><br><span class="line"><span class="string">主演：贾玲,张小斐,沈腾</span></span><br><span class="line"><span class="string">&lt;/p&gt;    </span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;div class=&quot;movie-item-info&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;name&quot;&gt;</span></span><br><span class="line"><span class="string">&lt;a title=&quot;刺杀，小说家&quot;&gt;刺杀，小说家&lt;/a&gt;</span></span><br><span class="line"><span class="string">&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;star&quot;&gt;</span></span><br><span class="line"><span class="string">主演：雷佳音,杨幂,董子健,于和伟</span></span><br><span class="line"><span class="string">&lt;/p&gt;    </span></span><br><span class="line"><span class="string">&lt;/div&gt; </span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 创建正则表达式对象， 分组提取title和star信息</span></span><br><span class="line">pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&lt;div.*?&lt;a title=&quot;(.*?)&quot;.*?star&quot;&gt;(.*?)&lt;/p.*?div&gt;&#x27;</span>, re.S)</span><br><span class="line">re_list = pattern.findall(html)</span><br><span class="line"><span class="built_in">print</span>(re_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 格式化输出</span></span><br><span class="line"><span class="keyword">if</span> re_list:</span><br><span class="line">    <span class="keyword">for</span> re_info <span class="keyword">in</span> re_list:</span><br><span class="line">        <span class="built_in">print</span>(<span class="number">20</span> * <span class="string">&#x27;-&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;电影名称：&#x27;</span>, re_info[<span class="number">0</span>])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;电影主演：&#x27;</span>, re_info[<span class="number">1</span>].strip())  <span class="comment"># 使用strip()函数去除主演字符串首尾的&#x27;\n&#x27;</span></span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;你好，李焕英&#x27;, &#x27;\n主演：贾玲,张小斐,沈腾\n&#x27;), (&#x27;刺杀，小说家&#x27;, &#x27;\n主演：雷佳音,杨幂,董子健,于和伟\n&#x27;)]</span><br><span class="line">--------------------</span><br><span class="line">电影名称： 你好，李焕英</span><br><span class="line">电影主演： 主演：贾玲,张小斐,沈腾</span><br><span class="line">--------------------</span><br><span class="line">电影名称： 刺杀，小说家</span><br><span class="line">电影主演： 主演：雷佳音,杨幂,董子健,于和伟</span><br></pre></td></tr></table></figure>

<h1 id="八、Python-csv模块（读写文件）"><a href="#八、Python-csv模块（读写文件）" class="headerlink" title="八、Python csv模块（读写文件）"></a>八、Python csv模块（读写文件）</h1><p>CSV 文件又称为逗号分隔值文件，是一种通用的、相对简单的文件格式，用以存储表格数据，包括数字或者字符。CSV 是电子表格和数据库中最常见的输入、输出文件格式，可参考《<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/CSV/10739?fr=aladdin">CSV介绍</a>》。</p>
<p>通过爬虫将数据抓取下来，然后把数据保存在文件，或者数据库中，这个过程称为数据的持久化存储。本节介绍 Python 内置模块 CSV 的读写操作。</p>
<h2 id="1-CSV文件写入"><a href="#1-CSV文件写入" class="headerlink" title="1)CSV文件写入"></a>1)CSV文件写入</h2><h4 id="1-csv-writer"><a href="#1-csv-writer" class="headerlink" title="1) csv.writer()"></a>1) csv.writer()</h4><p>csv 模块中的 writer 类可用于读写序列化的数据，其语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer(csvfile, dialect=&#x27;excel&#x27;, **fmtparams)</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li>csvfile：必须是支持迭代(Iterator)的对象，可以是文件(file)对象或者列表(list)对象。</li>
<li>dialect：编码风格，默认为 excel 的风格，也就是使用逗号<code>,</code>分隔。</li>
<li>fmtparam：格式化参数，用来覆盖之前 dialect 对象指定的编码风格。</li>
</ul>
<p>示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 操作文件对象时，需要添加newline参数逐行写入，否则会出现空行现象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;eggs.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    <span class="comment"># delimiter 指定分隔符，默认为逗号，这里指定为空格</span></span><br><span class="line">    <span class="comment"># quotechar 表示引用符</span></span><br><span class="line">    <span class="comment"># writerow 单行写入，列表格式传入数据</span></span><br><span class="line">    spamWriter = csv.writer(csvfile, delimiter=<span class="string">&#x27; &#x27;</span>, quotechar=<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">    spamWriter.writerow([<span class="string">&#x27;www.biancheng.net&#x27;</span>] * <span class="number">5</span> + [<span class="string">&#x27;how are you&#x27;</span>])</span><br><span class="line">    spamWriter.writerow([<span class="string">&#x27;hello world&#x27;</span>, <span class="string">&#x27;web site&#x27;</span>, <span class="string">&#x27;www.biancheng.net&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>eggs.csv 文件内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">www.biancheng.net www.biancheng.net www.biancheng.net www.biancheng.net www.biancheng.net |how are you|</span><br><span class="line">|hello world| |web site| www.biancheng.net</span><br></pre></td></tr></table></figure>

<p>其中，quotechar 是引用符，当一段话中出现分隔符的时候，用引用符将这句话括起来，以能排除歧义。</p>
<p>如果想同时写入多行数据，需要使用 writerrows() 方法，代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;aggs.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    writer = csv.writer(f)</span><br><span class="line">    <span class="comment"># 注意传入数据的格式为列表元组格式</span></span><br><span class="line">    writer.writerows([(<span class="string">&#x27;hello&#x27;</span>,<span class="string">&#x27;world&#x27;</span>), (<span class="string">&#x27;I&#x27;</span>,<span class="string">&#x27;love&#x27;</span>,<span class="string">&#x27;you&#x27;</span>)])</span><br></pre></td></tr></table></figure>

<p>aggs.csv文件内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hello,world</span><br><span class="line">I,love,you</span><br></pre></td></tr></table></figure>

<h4 id="2-csv-DictWriter"><a href="#2-csv-DictWriter" class="headerlink" title="2) csv.DictWriter()"></a>2) csv.DictWriter()</h4><p>当然也可使用 DictWriter 类以字典的形式读写数据，使用示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;names.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    <span class="comment"># 构建字段名称，也就是key</span></span><br><span class="line">    fieldnames = [<span class="string">&#x27;first_name&#x27;</span>, <span class="string">&#x27;last_name&#x27;</span>]</span><br><span class="line">    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)</span><br><span class="line">    <span class="comment"># 写入字段名，当做表头</span></span><br><span class="line">    writer.writeheader()</span><br><span class="line">    <span class="comment"># 多行写入</span></span><br><span class="line">    writer.writerows([&#123;<span class="string">&#x27;first_name&#x27;</span>: <span class="string">&#x27;Baked&#x27;</span>, <span class="string">&#x27;last_name&#x27;</span>: <span class="string">&#x27;Beans&#x27;</span>&#125;, &#123;<span class="string">&#x27;first_name&#x27;</span>: <span class="string">&#x27;Lovely&#x27;</span>, <span class="string">&#x27;last_name&#x27;</span>: <span class="string">&#x27;Spam&#x27;</span>&#125;])</span><br><span class="line">    <span class="comment"># 单行写入</span></span><br><span class="line">    writer.writerow(&#123;<span class="string">&#x27;first_name&#x27;</span>: <span class="string">&#x27;Wonderful&#x27;</span>, <span class="string">&#x27;last_name&#x27;</span>: <span class="string">&#x27;Spam&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>

<p>name.csv 文件内容，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">first_name,last_name</span><br><span class="line">Baked,Beans</span><br><span class="line">Lovely,Spam</span><br><span class="line">Wonderful,Spam</span><br></pre></td></tr></table></figure>

<h2 id="2-CSV文件读取"><a href="#2-CSV文件读取" class="headerlink" title="2)CSV文件读取"></a>2)CSV文件读取</h2><h4 id="1-csv-reader"><a href="#1-csv-reader" class="headerlink" title="1) csv.reader()"></a>1) csv.reader()</h4><p>csv 模块中的 reader 类和 DictReader 类用于读取文件中的数据，其中 reader() 语法格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csv.reader(csvfile, dialect=&#x27;excel&#x27;, **fmtparams)</span><br></pre></td></tr></table></figure>

<p>应用示例如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;eggs.csv&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    spamreader = csv.reader(csvfile, delimiter=<span class="string">&#x27; &#x27;</span>, quotechar=<span class="string">&#x27;|&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> spamreader:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;, &#x27;</span>.join(row))</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">www.biancheng.net, www.biancheng.net, www.biancheng.net, www.biancheng.net, www.biancheng.net, how are you</span><br><span class="line">hello world, web site, www.biancheng.net</span><br></pre></td></tr></table></figure>

<h4 id="2-csv-DictReader"><a href="#2-csv-DictReader" class="headerlink" title="2) csv.DictReader()"></a>2) csv.DictReader()</h4><p>应用示例如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;names.csv&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    reader = csv.DictReader(csvfile)</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">        <span class="built_in">print</span>(row[<span class="string">&#x27;first_name&#x27;</span>], row[<span class="string">&#x27;last_name&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Baked Beans</span><br><span class="line">Lovely Spam</span><br><span class="line">Wonderful Spam</span><br></pre></td></tr></table></figure>

<h1 id="九、【实例】Python爬虫抓取猫眼电影排行榜"><a href="#九、【实例】Python爬虫抓取猫眼电影排行榜" class="headerlink" title="九、【实例】Python爬虫抓取猫眼电影排行榜"></a>九、【实例】Python爬虫抓取猫眼电影排行榜</h1><p>本节使用 Python 爬虫抓取猫眼电影网 TOP100 排行榜（<a target="_blank" rel="noopener" href="https://maoyan.com/board/4%EF%BC%89%E5%BD%B1%E7%89%87%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%8C%85%E6%8B%AC%E7%94%B5%E5%BD%B1%E5%90%8D%E7%A7%B0%E3%80%81%E4%B8%8A%E6%98%A0%E6%97%B6%E9%97%B4%E3%80%81%E4%B8%BB%E6%BC%94%E4%BF%A1%E6%81%AF%E3%80%82">https://maoyan.com/board/4）影片信息，包括电影名称、上映时间、主演信息。</a></p>
<p>在开始编写程序之前，首先要确定页面类型（静态页面或动态页面），其次找出页面的 url 规律，最后通过分析网页元素结构来确定正则表达式，从而提取网页信息。</p>
<h2 id="1-确定页面类型"><a href="#1-确定页面类型" class="headerlink" title="1)确定页面类型"></a>1)确定页面类型</h2><p>点击右键查看页面源码，确定要抓取的数据是否存在于页面内。通过浏览得知要抓取的信息全部存在于源码内，因此该页面输属于静态页面。如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;movie-item-info&quot;&gt;        &lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/1200486&quot; title=&quot;我不是药神&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;&#123;movieId:1200486&#125;&quot;&gt;我不是药神&lt;/a&gt;&lt;/p&gt;        &lt;p class=&quot;star&quot;&gt;                主演：徐峥,周一围,王传君        &lt;/p&gt;&lt;p class=&quot;releasetime&quot;&gt;上映时间：2018-07-05&lt;/p&gt;    &lt;/div&gt;</span><br></pre></td></tr></table></figure>

<h2 id="2-确定url规律"><a href="#2-确定url规律" class="headerlink" title="2)确定url规律"></a>2)确定url规律</h2><p>想要确定 url 规律，需要您多浏览几个页面，然后才可以总结出 url 规律，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">第一页：https://maoyan.com/board/4?offset=0</span><br><span class="line">第二页：https://maoyan.com/board/4?offset=10</span><br><span class="line">第三页：https://maoyan.com/board/4?offset=20</span><br><span class="line">...</span><br><span class="line">第n页：https://maoyan.com/board/4?offset=(n-1)*10</span><br></pre></td></tr></table></figure>

<h3 id="1-确定正则表达式"><a href="#1-确定正则表达式" class="headerlink" title="1)确定正则表达式"></a>1)确定正则表达式</h3><p>通过分析网页元素结构来确定正则表达式，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;movie-item-info&quot;&gt;        &lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/1200486&quot; title=&quot;我不是药神&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;&#123;movieId:1200486&#125;&quot;&gt;我不是药神&lt;/a&gt;&lt;/p&gt;        &lt;p class=&quot;star&quot;&gt;                主演：徐峥,周一围,王传君        &lt;/p&gt;&lt;p class=&quot;releasetime&quot;&gt;上映时间：2018-07-05&lt;/p&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure>

<p>使用 Chrome 开发者调试工具来精准定位要抓取信息的元素结构。之所以这样做，是因为这能避免正则表达式的冗余，提高编写正则表达式的速度。正则表达式如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;movie-item-info&quot;&gt;.*?title=&quot;(.*?)&quot;.*?class=&quot;star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p&gt;</span><br></pre></td></tr></table></figure>

<p>编写正则表达式时将需要提取的信息使用<code>(.*?)</code>代替，而不需要的内容（包括元素标签）使用<code>.*?</code>代替。</p>
<h2 id="3-编写爬虫程序-1"><a href="#3-编写爬虫程序-1" class="headerlink" title="3)编写爬虫程序"></a>3)编写爬虫程序</h2><p>下面使用面向对象的方法编写爬虫程序，主要编写四个函数，分别是请求函数、解析函数、保存数据函数、主函数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> ua_info <span class="keyword">import</span> ua_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个爬虫类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MaoYanSpider</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url = <span class="string">&#x27;https://www.maoyan.com/board/4?offset=&#123;&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 请求函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_html</span>(<span class="params">self, url</span>):</span><br><span class="line">        headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: random.choice(ua_list)&#125;</span><br><span class="line">        <span class="comment"># req = request.Request(url, headers)</span></span><br><span class="line">        req = request.Request(url=url, headers=headers)</span><br><span class="line">        res = request.urlopen(req)</span><br><span class="line">        html = res.read().decode()</span><br><span class="line">        <span class="comment"># 调用解析函数</span></span><br><span class="line">        self.parse_html(html)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_html</span>(<span class="params">self, html</span>):</span><br><span class="line">        <span class="comment"># 正则表达式，并创建对象</span></span><br><span class="line">        pattern = <span class="string">&#x27;&lt;div class=&quot;movie-item-info&quot;&gt;.*?title=&quot;(.*?)&quot;.*?&quot;star&quot;&gt;(.*?)&lt;/p.*?&quot;releasetime&quot;&gt;(.*?)&lt;/p&gt;&#x27;</span></span><br><span class="line">        regex = re.<span class="built_in">compile</span>(pattern, re.S)</span><br><span class="line">        <span class="comment"># 列表元组</span></span><br><span class="line">        re_list = regex.findall(html)</span><br><span class="line">        <span class="built_in">print</span>(html)</span><br><span class="line">        <span class="built_in">print</span>(re_list)</span><br><span class="line">        <span class="comment"># 调用保存数据函数</span></span><br><span class="line">        self.save_html(re_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存数据函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_html</span>(<span class="params">self, re_list</span>):</span><br><span class="line">        <span class="comment"># 生成文件对象</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;猫眼电影.csv&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># 生成csv操作对象</span></span><br><span class="line">            writer = csv.writer(f)</span><br><span class="line">            <span class="comment"># 数据整理</span></span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> re_list:</span><br><span class="line">                name = r[<span class="number">0</span>].strip()</span><br><span class="line">                star = r[<span class="number">1</span>].strip()[<span class="number">3</span>:]</span><br><span class="line">                <span class="comment"># 切片获取上映时间 eg:上映时间：2018-07-05</span></span><br><span class="line">                time = r[<span class="number">2</span>].strip()[<span class="number">5</span>:<span class="number">15</span>]</span><br><span class="line">                Data = [name, star, time]</span><br><span class="line">                <span class="comment"># 写入csv文件</span></span><br><span class="line">                writer.writerow(Data)</span><br><span class="line">                <span class="built_in">print</span>(name, time, star)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 主函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 抓取第一页数据</span></span><br><span class="line">        <span class="keyword">for</span> offset <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">11</span>, <span class="number">10</span>):</span><br><span class="line">            url = self.url.<span class="built_in">format</span>(offset)</span><br><span class="line">            self.get_html(url)</span><br><span class="line">            <span class="comment"># 爬虫休眠--生成1-2之间的浮点数</span></span><br><span class="line">            time.sleep(random.uniform(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以脚本方式启动</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spider = MaoYanSpider()</span><br><span class="line">        spider.run()</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ERROR:&#x27;</span>, e)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<p>我不是药神 2018-07-05 徐峥,周一围,王传君<br>肖申克的救赎 1994-09-10 蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿<br>绿皮书 2019-03-01 维果·莫腾森,马赫沙拉·阿里,琳达·卡德里尼<br>海上钢琴师 2019-11-15 蒂姆·罗斯,比尔·努恩,克兰伦斯·威廉姆斯三世<br>小偷家族 2018-08-03 中川雅也,安藤樱,松冈茉优<br>霸王别姬 1993-07-26 张国荣,张丰毅,巩俐<br>哪吒之魔童降世 2019-07-26 吕艳婷,囧森瑟夫,瀚墨<br>美丽人生 2020-01-03 罗伯托·贝尼尼,朱斯蒂诺·杜拉诺,赛尔乔·比尼·布斯特里克<br>这个杀手不太冷 1994-09-14 让·雷诺,加里·奥德曼,娜塔莉·波特曼<br>盗梦空间 2010-09-01 莱昂纳多·迪卡普里奥,渡边谦,约瑟夫·高登-莱维特</p>
<h1 id="十、【实例】-Python-Pymysql实现数据存储"><a href="#十、【实例】-Python-Pymysql实现数据存储" class="headerlink" title="十、【实例】 Python Pymysql实现数据存储"></a>十、【实例】 Python Pymysql实现数据存储</h1><h1 id="十一、【实例】Python爬虫：抓取多级页面数据"><a href="#十一、【实例】Python爬虫：抓取多级页面数据" class="headerlink" title="十一、【实例】Python爬虫：抓取多级页面数据"></a>十一、【实例】Python爬虫：抓取多级页面数据</h1><h1 id="十二、【实例】Python-Requests库安装和使用"><a href="#十二、【实例】Python-Requests库安装和使用" class="headerlink" title="十二、【实例】Python Requests库安装和使用"></a>十二、【实例】Python Requests库安装和使用</h1><p>Python 提供了多个用来编写爬虫程序的库，除了前面已经介绍的 urllib 库之外，还有一个很重的 Requests 库，这个库的宗旨是“让 HTTP 服务于人类”。</p>
<p><img src="/images/loading/loading.gif" data-original="9-210R0113914536.gif" alt="Python requests"></p>
<p>Requests 是 Python 的第三方库，它的安装非常简便，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m pip install requests</span><br></pre></td></tr></table></figure>

<p>Requests 库是在 urllib 的基础上开发而来，它使用 Python 语言编写，并且采用了 Apache2 Licensed（一种开源协议）的 HTTP 库。与 urllib 相比，Requests 更加方便、快捷，因此在编写爬虫程序时 Requests 库使用较多。</p>
<h2 id="1-常用请求方法"><a href="#1-常用请求方法" class="headerlink" title="1) 常用请求方法"></a>1) 常用请求方法</h2><h4 id="1-requests-get"><a href="#1-requests-get" class="headerlink" title="1) requests.get()"></a>1) requests.get()</h4><p>该方法用于 GET 请求，表示向网站发起请求，获取页面响应对象。语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">res = requests.get(url,headers=headers,params,timeout)</span><br></pre></td></tr></table></figure>

<p>参数说明如下：</p>
<ul>
<li>url：要抓取的 url 地址。</li>
<li>headers：用于包装请求头信息。</li>
<li>params：请求时携带的查询字符串参数。</li>
<li>timeout：超时时间，超过时间会抛出异常。</li>
</ul>
<p>具体使用示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import requestsurl = &#x27;http://baidu.com&#x27;response = requests.get(url)print(response)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;Response [200]&gt;</span><br></pre></td></tr></table></figure>

<p>获取带查询字符串参数的响应对象，如下所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;编程帮&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;url&#x27;</span>: <span class="string">&quot;www.biancheng.net&quot;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, params=data)</span><br><span class="line"><span class="comment">#直接拼接参数也可以</span></span><br><span class="line"><span class="comment">#response = requests.get(http://httpbin.org/get?name=gemey&amp;age=22)</span></span><br><span class="line"><span class="comment">#调用响应对象text属性，获取文本信息</span></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;\u7f16\u7a0b\u5e2e&quot;,</span><br><span class="line">    &quot;url&quot;: &quot;www.biancheng.net&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*/*&quot;,</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests/2.23.0&quot;,</span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root=1-60420026-236f9205646b68706d0fafa7&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;origin&quot;: &quot;121.17.25.194&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;http://httpbin.org/get?name=\u7f16\u7a0b\u5e2e&amp;url=www.biancheng.net&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-requests-post"><a href="#2-requests-post" class="headerlink" title="2) requests.post()"></a>2) requests.post()</h4><p>该方法用于 POST 请求，先由用户向目标 url 提交数据，然后服务器返回一个 HttpResponse 响应对象，语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response=requests.post(url,data=&#123;请求体的字典&#125;)</span><br></pre></td></tr></table></figure>

<p>示例如下所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#百度翻译</span></span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com&#x27;</span></span><br><span class="line"><span class="comment">#post请求体携带的参数，可通过开发者调试工具查看</span></span><br><span class="line"><span class="comment">#查看步骤：NetWork选项-&gt;Headers选项-&gt;Form Data</span></span><br><span class="line">data = &#123;<span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;zh&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;to&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;query&#x27;</span>: <span class="string">&#x27;编程帮www.biancheng.net你好&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">response = requests.post(url, data=data)</span><br><span class="line"><span class="built_in">print</span>(response)</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;Response [200]&gt;</span><br></pre></td></tr></table></figure>

<p>查看 Form Data 的步骤，如下图所示：</p>
<p><img src="/images/loading/loading.gif" data-original="9-210Q9131356309.gif" alt="Python爬重开发者工具使用"></p>
<p>图1：Chrome开发者调试工具（<a target="_blank" rel="noopener" href="http://c.biancheng.net/uploads/allimg/210819/9-210Q9131356309.gif">点击看高清图</a>）</p>
<h2 id="2-对象属性"><a href="#2-对象属性" class="headerlink" title="2) 对象属性"></a>2) 对象属性</h2><p>当我们使用 Requests 模块向一个 URL 发起请求后会返回一个 HttpResponse 响应对象，该对象具有以下常用属性：</p>
<table>
<thead>
<tr>
<th>常用属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>encoding</td>
<td>查看或者指定响应字符编码</td>
</tr>
<tr>
<td>status_code</td>
<td>返回HTTP响应码</td>
</tr>
<tr>
<td>url</td>
<td>查看请求的 url 地址</td>
</tr>
<tr>
<td>headers</td>
<td>查看请求头信息</td>
</tr>
<tr>
<td>cookies</td>
<td>查看cookies 信息</td>
</tr>
<tr>
<td>text</td>
<td>以字符串形式输出</td>
</tr>
<tr>
<td>content</td>
<td>以字节流形式输出，若要保存下载图片需使用该属性。</td>
</tr>
</tbody></table>
<p>使用示例如下所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">response = requests.get(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.encoding)</span><br><span class="line">response.encoding=<span class="string">&quot;utf-8&quot;</span>    <span class="comment">#更改为utf-8编码</span></span><br><span class="line"><span class="built_in">print</span>(response.status_code)  <span class="comment"># 打印状态码</span></span><br><span class="line"><span class="built_in">print</span>(response.url)          <span class="comment"># 打印请求url</span></span><br><span class="line"><span class="built_in">print</span>(response.headers)      <span class="comment"># 打印头信息</span></span><br><span class="line"><span class="built_in">print</span>(response.cookies)      <span class="comment"># 打印cookie信息</span></span><br><span class="line"><span class="built_in">print</span>(response.text)  <span class="comment">#以字符串形式打印网页源码</span></span><br><span class="line"><span class="built_in">print</span>(response.content) <span class="comment">#以字节流形式打印</span></span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#编码格式</span><br><span class="line">ISO-8859-1</span><br><span class="line">#响应码</span><br><span class="line">200</span><br><span class="line">#url地址</span><br><span class="line">http://www.baidu.com/</span><br><span class="line">#请求头信息</span><br><span class="line">&#123;&#x27;Cache-Control&#x27;: &#x27;private, no-cache, no-store, proxy-revalidate, no-transform&#x27;, &#x27;Connection&#x27;: &#x27;keep-alive&#x27;, &#x27;Content-Encoding&#x27;: &#x27;gzip&#x27;, &#x27;Content-Type&#x27;: &#x27;text/html&#x27;, &#x27;Date&#x27;: &#x27;Mon, 08 Mar 2021 05:19:33 GMT&#x27;, &#x27;Last-Modified&#x27;: &#x27;Mon, 23 Jan 2017 13:27:29 GMT&#x27;, &#x27;Pragma&#x27;: &#x27;no-cache&#x27;, &#x27;Server&#x27;: &#x27;bfe/1.0.8.18&#x27;, &#x27;Set-Cookie&#x27;: &#x27;BDORZ=27315; max-age=86400; domain=.baidu.com; path=/&#x27;, &#x27;Transfer-Encoding&#x27;: &#x27;chunked&#x27;&#125;</span><br><span class="line">#查看cookies信息</span><br><span class="line">&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;</span><br><span class="line">...内容过长，此处省略后两项输出</span><br></pre></td></tr></table></figure>

<h2 id="3-Requests库应用"><a href="#3-Requests库应用" class="headerlink" title="3) Requests库应用"></a>3) Requests库应用</h2><p>示例应用：使用 Requsets 库下载百度图片。</p>
<p>首先打开百度图片（<a target="_blank" rel="noopener" href="https://image.baidu.com/%EF%BC%89%EF%BC%8C%E5%B9%B6%E5%9C%A8%E8%BE%93%E5%85%A5%E6%A1%86%E6%90%9C%E7%B4%A2">https://image.baidu.com/），并在输入框搜索</a> “python logo”，然后使用 Chrome 开发者工具查看第一张图片的源地址，即 data-imgurl 所对应的 url 地址，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data-imgurl=&quot;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=38785274,1357847304&amp;fm=26&amp;gp=0.jpg&quot;</span><br></pre></td></tr></table></figure>

<p>可以将上述 url 粘贴至浏览器地址栏进行验证。当我们确定图片地址后，就可以使用 requests 库进行编码了：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=38785274,1357847304&amp;fm=26&amp;gp=0.jpg&#x27;</span></span><br><span class="line"><span class="comment"># 简单定义浏览器ua信息</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/4.0&#x27;</span>&#125;</span><br><span class="line"><span class="comment"># 读取图片需要使用content属性</span></span><br><span class="line">html = requests.get(url=url, headers=headers).content</span><br><span class="line"><span class="comment"># 以二进制的方式下载图片</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;C:/Users/Administrator/Desktop/image/python_logo.jpg&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(html)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后，您会在桌面文件夹中找到已经下载好的图片</p>
<h1 id="十三、【实例】Python爬虫抓取网络照片"><a href="#十三、【实例】Python爬虫抓取网络照片" class="headerlink" title="十三、【实例】Python爬虫抓取网络照片"></a>十三、【实例】Python爬虫抓取网络照片</h1><p>本节编写一个快速下载照片的程序，通过百度图片下载您想要的前 60 张图片，并将其保存至相应的目录。本节实战案例是上一节《<a target="_blank" rel="noopener" href="http://c.biancheng.net/python_spider/requests.html">Python Request库安装和使用</a>》图片下载案例的延伸。</p>
<h2 id="1-分析url规律"><a href="#1-分析url规律" class="headerlink" title="1) 分析url规律"></a>1) 分析url规律</h2><p>打开百度图片翻页版（<a target="_blank" rel="noopener" href="http://image.baidu.com/search/flip?tn=baiduimage&ie=utf-8&word=python&pn=0&gsm=50&ct=&ic=0&lm=-1&width=0&height=0">点击访问</a>），该翻页版网址要妥善保留。其 url 规律如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">第一页：https://image.baidu.com/search/flip?tn=baiduimage&amp;word=python&amp;pn=0</span><br><span class="line">第二页：https://image.baidu.com/search/flip?tn=baiduimage&amp;word=python&amp;pn=20</span><br><span class="line">第三页：https://image.baidu.com/search/flip?tn=baiduimage&amp;word=python&amp;pn=40</span><br><span class="line">第n页：https://image.baidu.com/search/flip?tn=baiduimage&amp;word=python&amp;pn=20*（n-1)</span><br></pre></td></tr></table></figure>

<p>百度为了限制爬虫，将原来的翻页版变为了“瀑布流”浏览形式，也就是通过滚动滑轮自动加载图片，此种方式在一定程度上限制了爬虫程序。</p>
<h2 id="2-写正则表达式"><a href="#2-写正则表达式" class="headerlink" title="2) 写正则表达式"></a>2) 写正则表达式</h2><p>通过上一节可以得知每一张图片有一个源地址如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data-imgurl=&quot;图片源地址&quot;</span><br></pre></td></tr></table></figure>

<p>复制图片源地址，并检查网页源代码，使用 Ctrl+F 搜索该地址，如下图所示：</p>
<p><img src="/images/loading/loading.gif" data-original="9-210Q91336203T.png" alt="request模块使用"></p>
<p>图1：检查网页结构（<a target="_blank" rel="noopener" href="http://c.biancheng.net/uploads/allimg/210819/9-210Q91336203T.png">点击看高清图</a>）</p>
<p>使用上述方式依次检查几张图片，您会发现每张图片源地址，有如下三种匹配结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;thumbURL&quot;:&quot;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=38785274,1357847304&amp;fm=26&amp;gp=0.jpg&quot;</span><br><span class="line">&quot;middleURL&quot;:&quot;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=38785274,1357847304&amp;fm=26&amp;gp=0.jpg&quot;</span><br><span class="line">&quot;hoverURL&quot;:&quot;https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=38785274,1357847304&amp;fm=26&amp;gp=0.jpg&quot;</span><br></pre></td></tr></table></figure>

<p>任选其一，写出图片源地址正则表达式，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re_bds=&#x27;&quot;hoverURL&quot;:&quot;(.*?)&quot;&#x27;</span><br></pre></td></tr></table></figure>

<h2 id="3-编写程序代码"><a href="#3-编写程序代码" class="headerlink" title="3) 编写程序代码"></a>3) 编写程序代码</h2><p>下面使用 Requests 库的相应方法和属性编写程序代码，最终实现一个快速下载照片的小程序。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf8 -*-</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduImageSpider</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url = <span class="string">&#x27;https://image.baidu.com/search/flip?tn=baiduimage&amp;word=&#123;&#125;&#x27;</span></span><br><span class="line">        self.headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/4.0&#x27;</span>&#125;</span><br><span class="line">    <span class="comment"># 获取图片</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_image</span>(<span class="params">self,url,word</span>):</span><br><span class="line">        <span class="comment">#使用 requests模块得到响应对象</span></span><br><span class="line">        res= requests.get(url,headers=self.headers)</span><br><span class="line">        <span class="comment"># 更改编码格式</span></span><br><span class="line">        res.encoding=<span class="string">&quot;utf-8&quot;</span></span><br><span class="line">        <span class="comment"># 得到html网页</span></span><br><span class="line">        html=res.text</span><br><span class="line">        <span class="built_in">print</span>(html)</span><br><span class="line">        <span class="comment">#正则解析</span></span><br><span class="line">        pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;&quot;hoverURL&quot;:&quot;(.*?)&quot;&#x27;</span>,re.S)</span><br><span class="line">        img_link_list = pattern.findall(html)</span><br><span class="line">        <span class="comment">#存储图片的url链接 </span></span><br><span class="line">        <span class="built_in">print</span>(img_link_list)</span><br><span class="line">        <span class="comment"># 创建目录，用于保存图片</span></span><br><span class="line">        directory = <span class="string">&#x27;C:/Users/Administrator/Desktop/image/&#123;&#125;/&#x27;</span>.<span class="built_in">format</span>(word)</span><br><span class="line">        <span class="comment"># 如果目录不存在则创建，此方法常用</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(directory):</span><br><span class="line">            os.makedirs(directory)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#添加计数 </span></span><br><span class="line">        i = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> img_link <span class="keyword">in</span> img_link_list:</span><br><span class="line">            filename = <span class="string">&#x27;&#123;&#125;&#123;&#125;_&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(directory, word, i)</span><br><span class="line">            self.save_image(img_link,filename)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">    <span class="comment">#下载图片</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_image</span>(<span class="params">self,img_link,filename</span>):</span><br><span class="line">        html = requests.get(url=img_link,headers=self.headers).content</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(html)</span><br><span class="line">        <span class="built_in">print</span>(filename,<span class="string">&#x27;下载成功&#x27;</span>)</span><br><span class="line">    <span class="comment"># 入口函数 </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        word = <span class="built_in">input</span>(<span class="string">&quot;您想要谁的照片？&quot;</span>)</span><br><span class="line">        word_parse = parse.quote(word)</span><br><span class="line">        url = self.url.<span class="built_in">format</span>(word_parse)</span><br><span class="line">        self.get_image(url,word)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spider = BaiduImageSpider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure>

<p>程序执行结果如下图：</p>
<p><img src="/images/loading/loading.gif" data-original="9-210Q91336322E.png" alt="程序执行结果"><br>图2：程序执行图</p>
<p>目录文件下载图如下所示：</p>
<p><img src="/images/loading/loading.gif" data-original="9-210Q9133642311.png" alt="python爬虫实战"><br>图3：程序执行结果</p>
<h1 id="Python-Selenium"><a href="#Python-Selenium" class="headerlink" title="Python Selenium"></a>Python Selenium</h1><h2 id="1-【实例】抓取京东商城信息"><a href="#1-【实例】抓取京东商城信息" class="headerlink" title="1.【实例】抓取京东商城信息"></a>1.【实例】抓取京东商城信息</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf8</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">JdSpider</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.url = <span class="string">&#x27;http://www.jd.com/&#x27;</span></span><br><span class="line">        self.options = webdriver.ChromeOptions()  <span class="comment"># 无头模式</span></span><br><span class="line">        self.options.add_argument(<span class="string">&#x27;--headless&#x27;</span>)</span><br><span class="line">        self.browser = webdriver.Chrome(options=self.options)  <span class="comment"># 创建无界面参数的浏览器对象</span></span><br><span class="line">        self.i = <span class="number">0</span>  <span class="comment"># 计数，一共有多少件商品</span></span><br><span class="line">        <span class="comment"># 输入地址+输入商品+点击按钮，切记这里元素节点是京东首页的输入栏、搜索按钮</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_html</span>(<span class="params">self</span>):</span><br><span class="line">        self.browser.get(self.url)</span><br><span class="line">        self.browser.find_element(By.XPATH, value=<span class="string">&#x27;//*[@id=&quot;key&quot;]&#x27;</span>).send_keys(<span class="string">&#x27;python书籍&#x27;</span>)</span><br><span class="line">        self.browser.find_element(By.XPATH, value=<span class="string">&quot;//*[@class=&#x27;form&#x27;]/button&quot;</span>).click()</span><br><span class="line">        <span class="comment"># 把进度条件拉倒最底部+提取商品信息</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_data</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 执行js语句，拉动进度条件</span></span><br><span class="line">        self.browser.execute_script(</span><br><span class="line">            <span class="string">&#x27;window.scrollTo(0,document.body.scrollHeight)&#x27;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 给页面元素加载时预留时间</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 用 xpath 提取每页中所有商品，最终形成一个大列表</span></span><br><span class="line">        li_list = self.browser.find_elements(By.XPATH, value=<span class="string">&#x27;//*[@id=&quot;J_goodsList&quot;]/ul/li&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            <span class="comment"># 构建空字典</span></span><br><span class="line">            item = &#123;&#125;</span><br><span class="line">            item[<span class="string">&#x27;name&#x27;</span>] = li.find_element(By.XPATH, value=<span class="string">&#x27;.//div[@class=&quot;p-name&quot;]/a/em&#x27;</span>).text.strip()</span><br><span class="line">            item[<span class="string">&#x27;price&#x27;</span>] = li.find_element(By.XPATH, value=<span class="string">&#x27;.//div[@class=&quot;p-price&quot;]&#x27;</span>).text.strip()</span><br><span class="line">            item[<span class="string">&#x27;count&#x27;</span>] = li.find_element(By.XPATH, value=<span class="string">&#x27;.//div[@class=&quot;p-commit&quot;]/strong&#x27;</span>).text.strip()</span><br><span class="line">            item[<span class="string">&#x27;shop&#x27;</span>] = li.find_element(By.XPATH, value=<span class="string">&#x27;.//div[@class=&quot;p-shopnum&quot;]&#x27;</span>).text.strip()</span><br><span class="line">            <span class="built_in">print</span>(item)</span><br><span class="line">            self.i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 搜索出想要抓取商品的页面</span></span><br><span class="line">        self.get_html()</span><br><span class="line">        <span class="comment"># 循环执行点击“下一页”操作</span></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 获取每一页要抓取的数据</span></span><br><span class="line">            self.get_data()</span><br><span class="line">            <span class="comment"># 判断是否是最一页</span></span><br><span class="line">            <span class="keyword">if</span> self.browser.page_source.find(<span class="string">&#x27;pn-next disabled&#x27;</span>) == -<span class="number">1</span>:</span><br><span class="line">                self.browser.find_element(By.CLASS_NAME, value=<span class="string">&#x27;pn-next&#x27;</span>).click()</span><br><span class="line">                <span class="comment"># 预留元素加载时间</span></span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;数量&#x27;</span>, self.i)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spider = JdSpider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python-%E7%88%AC%E8%99%AB/" rel="tag"># Python,爬虫</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/09/22/CMC/" rel="prev" title="全国大学生数学竞赛笔记">
      <i class="fa fa-chevron-left"></i> 全国大学生数学竞赛笔记
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Python%E7%88%AC%E8%99%AB"><span class="nav-number">1.</span> <span class="nav-text">Python爬虫</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%AC%AC%E4%B8%80%E4%B8%AAPython%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F-%E8%8E%B7%E5%8F%96%E7%BD%91%E9%A1%B5html%E4%BF%A1%E6%81%AF"><span class="nav-number">2.</span> <span class="nav-text">一、第一个Python爬虫程序-获取网页html信息</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%8E%B7%E5%8F%96%E5%93%8D%E5%BA%94%E5%AF%B9%E8%B1%A1"><span class="nav-number">2.1.</span> <span class="nav-text">1) 获取响应对象</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E8%BE%93%E5%87%BAHTML%E4%BF%A1%E6%81%AF"><span class="nav-number">2.2.</span> <span class="nav-text">2) 输出HTML信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">2.3.</span> <span class="nav-text">3) 常用方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-urlopen"><span class="nav-number">2.3.0.1.</span> <span class="nav-text">1) urlopen()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Request"><span class="nav-number">2.3.0.2.</span> <span class="nav-text">2) Request()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-html%E5%93%8D%E5%BA%94%E5%AF%B9%E8%B1%A1%E6%96%B9%E6%B3%95"><span class="nav-number">2.3.0.3.</span> <span class="nav-text">3) html响应对象方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81%E6%93%8D%E4%BD%9C"><span class="nav-number">2.3.0.4.</span> <span class="nav-text">4) 编码解码操作</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E3%80%81User-Agent%EF%BC%88%E7%94%A8%E6%88%B7%E4%BB%A3%E7%90%86%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">二、User-Agent（用户代理）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8FUA%E4%BF%A1%E6%81%AF"><span class="nav-number">3.1.</span> <span class="nav-text">1)爬虫程序UA信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E9%87%8D%E6%9E%84%E7%88%AC%E8%99%ABUA%E4%BF%A1%E6%81%AF"><span class="nav-number">3.2.</span> <span class="nav-text">2)重构爬虫UA信息</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E6%9E%84%E5%BB%BAUser-Agnet%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-number">4.</span> <span class="nav-text">三、构建User-Agnet代理池</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%87%AA%E5%AE%9A%E4%B9%89UA%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="nav-number">4.1.</span> <span class="nav-text">1)自定义UA代理池</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%A8%A1%E5%9D%97%E9%9A%8F%E6%9C%BA%E8%8E%B7%E5%8F%96UA"><span class="nav-number">4.2.</span> <span class="nav-text">2)模块随机获取UA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-URL%E7%BC%96%E7%A0%81-x2F-%E8%A7%A3%E7%A0%81%E8%AF%A6%E8%A7%A3"><span class="nav-number">4.3.</span> <span class="nav-text">3)URL编码&#x2F;解码详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-URL%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90"><span class="nav-number">4.3.1.</span> <span class="nav-text">3.1 URL基本组成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E5%93%AA%E4%BA%9B%E5%AD%97%E7%AC%A6%E9%9C%80%E8%A6%81%E7%BC%96%E7%A0%81"><span class="nav-number">4.3.2.</span> <span class="nav-text">3.2 哪些字符需要编码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Python%E5%AE%9E%E7%8E%B0%E7%BC%96%E7%A0%81%E4%B8%8E%E8%A7%A3%E7%A0%81"><span class="nav-number">4.3.3.</span> <span class="nav-text">3.3 Python实现编码与解码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E7%BC%96%E7%A0%81urlencode"><span class="nav-number">4.3.3.1.</span> <span class="nav-text">1) 编码urlencode()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%A7%A3%E7%A0%81unquote-string"><span class="nav-number">4.3.3.2.</span> <span class="nav-text">2) 解码unquote(string)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-URL%E5%9C%B0%E5%9D%80%E6%8B%BC%E6%8E%A5%E6%96%B9%E5%BC%8F"><span class="nav-number">4.3.3.3.</span> <span class="nav-text">3) URL地址拼接方式</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91Python%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%E7%BD%91%E9%A1%B5"><span class="nav-number">5.</span> <span class="nav-text">四、【实例】Python爬虫抓取网页</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%AF%BC%E5%85%A5%E6%89%80%E9%9C%80%E6%A8%A1%E5%9D%97"><span class="nav-number">5.1.</span> <span class="nav-text">1)导入所需模块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%8B%BC%E6%8E%A5URL%E5%9C%B0%E5%9D%80"><span class="nav-number">5.2.</span> <span class="nav-text">2)拼接URL地址</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%90%91URL%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82"><span class="nav-number">5.3.</span> <span class="nav-text">3)向URL发送请求</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E4%BF%9D%E5%AD%98%E4%B8%BA%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6"><span class="nav-number">5.4.</span> <span class="nav-text">4)保存为本地文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E4%BF%AE%E6%94%B9%E7%A8%8B%E5%BA%8F"><span class="nav-number">5.5.</span> <span class="nav-text">5)函数式编程修改程序</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91Python%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%E7%99%BE%E5%BA%A6%E8%B4%B4%E5%90%A7%E6%95%B0%E6%8D%AE"><span class="nav-number">6.</span> <span class="nav-text">五、【实例】Python爬虫抓取百度贴吧数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%88%A4%E6%96%AD%E9%A1%B5%E9%9D%A2%E7%B1%BB%E5%9E%8B"><span class="nav-number">6.1.</span> <span class="nav-text">1)判断页面类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%AF%BB%E6%89%BEURL%E5%8F%98%E5%8C%96%E8%A7%84%E5%BE%8B"><span class="nav-number">6.2.</span> <span class="nav-text">2)寻找URL变化规律</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%BC%96%E5%86%99%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F"><span class="nav-number">6.3.</span> <span class="nav-text">3)编写爬虫程序</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%AF%B7%E6%B1%82%E5%87%BD%E6%95%B0"><span class="nav-number">6.3.0.1.</span> <span class="nav-text">1) 请求函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E8%A7%A3%E6%9E%90%E5%87%BD%E6%95%B0"><span class="nav-number">6.3.0.2.</span> <span class="nav-text">2) 解析函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE%E5%87%BD%E6%95%B0"><span class="nav-number">6.3.0.3.</span> <span class="nav-text">3) 保存数据函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%85%A5%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="nav-number">6.3.0.4.</span> <span class="nav-text">4) 入口函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F%E7%BB%93%E6%9E%84"><span class="nav-number">6.4.</span> <span class="nav-text">4)爬虫程序结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F%E9%9A%8F%E6%9C%BA%E4%BC%91%E7%9C%A0"><span class="nav-number">6.5.</span> <span class="nav-text">5)爬虫程序随机休眠</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="nav-number">7.</span> <span class="nav-text">六、正则表达式基本语法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%85%83%E5%AD%97%E7%AC%A6"><span class="nav-number">7.1.</span> <span class="nav-text">1)正则表达式元字符</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%85%83%E5%AD%97%E7%AC%A6"><span class="nav-number">7.1.0.1.</span> <span class="nav-text">1) 元字符</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%87%8F%E8%AF%8D"><span class="nav-number">7.1.0.2.</span> <span class="nav-text">2) 量词</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%AD%97%E7%AC%A6%E7%BB%84"><span class="nav-number">7.1.0.3.</span> <span class="nav-text">3) 字符组</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F%E9%9D%9E%E8%B4%AA%E5%A9%AA%E6%A8%A1%E5%BC%8F"><span class="nav-number">7.2.</span> <span class="nav-text">2)贪婪模式非贪婪模式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%BD%AC%E4%B9%89"><span class="nav-number">7.3.</span> <span class="nav-text">3)正则表达式转义</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%83%E3%80%81Python-re%E6%A8%A1%E5%9D%97%E7%94%A8%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="nav-number">8.</span> <span class="nav-text">七、Python re模块用法详解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-re%E6%A8%A1%E5%9D%97%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="nav-number">8.1.</span> <span class="nav-text">1)re模块常用方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-re-compile"><span class="nav-number">8.1.0.1.</span> <span class="nav-text">1) re.compile()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-re-findall"><span class="nav-number">8.1.0.2.</span> <span class="nav-text">2) re.findall()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-regex-findall"><span class="nav-number">8.1.0.3.</span> <span class="nav-text">3) regex.findall()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-re-split"><span class="nav-number">8.1.0.4.</span> <span class="nav-text">4) re.split()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-re-search"><span class="nav-number">8.1.0.5.</span> <span class="nav-text">5) re.search()</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-flags%E5%8A%9F%E8%83%BD%E6%A0%87%E5%BF%97%E4%BD%8D"><span class="nav-number">8.2.</span> <span class="nav-text">2)flags功能标志位</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%88%86%E7%BB%84"><span class="nav-number">8.3.</span> <span class="nav-text">3)正则表达式分组</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E7%BD%91%E9%A1%B5%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96"><span class="nav-number">8.4.</span> <span class="nav-text">4)网页信息提取</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%AB%E3%80%81Python-csv%E6%A8%A1%E5%9D%97%EF%BC%88%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6%EF%BC%89"><span class="nav-number">9.</span> <span class="nav-text">八、Python csv模块（读写文件）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-CSV%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5"><span class="nav-number">9.1.</span> <span class="nav-text">1)CSV文件写入</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-csv-writer"><span class="nav-number">9.1.0.1.</span> <span class="nav-text">1) csv.writer()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-csv-DictWriter"><span class="nav-number">9.1.0.2.</span> <span class="nav-text">2) csv.DictWriter()</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-CSV%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96"><span class="nav-number">9.2.</span> <span class="nav-text">2)CSV文件读取</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-csv-reader"><span class="nav-number">9.2.0.1.</span> <span class="nav-text">1) csv.reader()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-csv-DictReader"><span class="nav-number">9.2.0.2.</span> <span class="nav-text">2) csv.DictReader()</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B9%9D%E3%80%81%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91Python%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%E7%8C%AB%E7%9C%BC%E7%94%B5%E5%BD%B1%E6%8E%92%E8%A1%8C%E6%A6%9C"><span class="nav-number">10.</span> <span class="nav-text">九、【实例】Python爬虫抓取猫眼电影排行榜</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E7%A1%AE%E5%AE%9A%E9%A1%B5%E9%9D%A2%E7%B1%BB%E5%9E%8B"><span class="nav-number">10.1.</span> <span class="nav-text">1)确定页面类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%A1%AE%E5%AE%9Aurl%E8%A7%84%E5%BE%8B"><span class="nav-number">10.2.</span> <span class="nav-text">2)确定url规律</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%A1%AE%E5%AE%9A%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">10.2.1.</span> <span class="nav-text">1)确定正则表达式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%BC%96%E5%86%99%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F-1"><span class="nav-number">10.3.</span> <span class="nav-text">3)编写爬虫程序</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E3%80%81%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91-Python-Pymysql%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-number">11.</span> <span class="nav-text">十、【实例】 Python Pymysql实现数据存储</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%B8%80%E3%80%81%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91Python%E7%88%AC%E8%99%AB%EF%BC%9A%E6%8A%93%E5%8F%96%E5%A4%9A%E7%BA%A7%E9%A1%B5%E9%9D%A2%E6%95%B0%E6%8D%AE"><span class="nav-number">12.</span> <span class="nav-text">十一、【实例】Python爬虫：抓取多级页面数据</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%BA%8C%E3%80%81%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91Python-Requests%E5%BA%93%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8"><span class="nav-number">13.</span> <span class="nav-text">十二、【实例】Python Requests库安装和使用</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%B8%B8%E7%94%A8%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95"><span class="nav-number">13.1.</span> <span class="nav-text">1) 常用请求方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-requests-get"><span class="nav-number">13.1.0.1.</span> <span class="nav-text">1) requests.get()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-requests-post"><span class="nav-number">13.1.0.2.</span> <span class="nav-text">2) requests.post()</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%AF%B9%E8%B1%A1%E5%B1%9E%E6%80%A7"><span class="nav-number">13.2.</span> <span class="nav-text">2) 对象属性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Requests%E5%BA%93%E5%BA%94%E7%94%A8"><span class="nav-number">13.3.</span> <span class="nav-text">3) Requests库应用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8D%81%E4%B8%89%E3%80%81%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91Python%E7%88%AC%E8%99%AB%E6%8A%93%E5%8F%96%E7%BD%91%E7%BB%9C%E7%85%A7%E7%89%87"><span class="nav-number">14.</span> <span class="nav-text">十三、【实例】Python爬虫抓取网络照片</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%88%86%E6%9E%90url%E8%A7%84%E5%BE%8B"><span class="nav-number">14.1.</span> <span class="nav-text">1) 分析url规律</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%86%99%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F"><span class="nav-number">14.2.</span> <span class="nav-text">2) 写正则表达式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81"><span class="nav-number">14.3.</span> <span class="nav-text">3) 编写程序代码</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Python-Selenium"><span class="nav-number">15.</span> <span class="nav-text">Python Selenium</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E3%80%90%E5%AE%9E%E4%BE%8B%E3%80%91%E6%8A%93%E5%8F%96%E4%BA%AC%E4%B8%9C%E5%95%86%E5%9F%8E%E4%BF%A1%E6%81%AF"><span class="nav-number">15.1.</span> <span class="nav-text">1.【实例】抓取京东商城信息</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiaqi Song"
      src="/images/wallhaven-o38w67.jpg">
  <p class="site-author-name" itemprop="name">Jiaqi Song</p>
  <div class="site-description" itemprop="description">人生，总会有不期而遇的温暖，和生生不息的希望。不管前方的路有多苦，只要走的方向正确，不管多么崎岖不平，都比站在原地更接近幸福。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/PgmTop" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;PgmTop" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://gitee.com/wind-9426" title="Gitee → https:&#x2F;&#x2F;gitee.com&#x2F;wind-9426" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>Gitee</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2117534690@qq.com" title="E-Mail → mailto:2117534690@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022.8.17 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiaqi Song</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>
    <script defer src="/lib/three/three-waves.min.js"></script>
    <script defer src="/lib/three/canvas_lines.min.js"></script>
    <script defer src="/lib/three/canvas_sphere.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'RxI6pGALkB3EJtH9xcTzjauB-gzGzoHsz',
      appKey     : 'yG4ahqMc2bOemFmi35x68X7h',
      placeholder: "发一条友善的评论，不发没关系，请继续保持友善哦~",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

<script>
            window.imageLazyLoadSetting = {
                isSPA: true,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var e=n.imageLazyLoadSetting.isSPA,i=n.imageLazyLoadSetting.preloadRatio||1,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight*i||document.documentElement.clientHeight*i)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},t.src!==i&&(n.src=i)}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body>
</html>
